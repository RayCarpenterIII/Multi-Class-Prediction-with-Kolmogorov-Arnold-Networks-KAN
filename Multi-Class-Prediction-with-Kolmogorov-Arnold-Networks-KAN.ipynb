{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00881cd0-8959-4818-9470-89d4445e561b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# KAN For Multivariate Classification in Predicting Visual Stimuli Given Firing Rates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638a7735-f5e6-403a-8b1b-dad055d6f988",
   "metadata": {},
   "source": [
    "This notebook applies the Kolmogorov-Arnold Networks (KAN) architecture to visual stimulus prediction in mice, given the firing rates of single neurons.\n",
    "The model predicts what image the mouse is being shown out of 118 images. The data consists of spike-trains containing the total spikes each neuron fired when each image was shown, with around 2000 neurons each. \n",
    "\n",
    "This model's highest accuracy was ~75%, where an accuracy of ~0.85% would be a total guess. The model was found to work best with only one KAN layer connecting the input and output dimensions. This means only one activation function was found to transform the dataset. Although running PCA before this layer, with principal components roughly the same number of components as the number of classes, allowed the model to perform faster and with a diminished accuracy. This performed worse than models, like LSTMs or ST-GATs, that can account for temporal and spatial dependencies. However, the ability to show inference between the effect of individual neurons and the possibility to network effects might create a niche for Kolmogorov-Arnold Networks to be used in future studies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0594f691-ab30-406e-8d47-a29212ec742b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Best Model - KAN with one layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166e3055-9757-46ef-acc7-0bc7850edf4f",
   "metadata": {},
   "source": [
    "There is only an input and output layer with no hidden dimension here. This means only one singular activation function was found to predict the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff87a446-91ce-457f-ac85-76183d3b971b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated version 3!\n",
      "Initializing workflow...\n",
      "Loading existing datasets...\n",
      "Loaded spike trains dataset: <class 'pandas.core.frame.DataFrame'>\n",
      "Total time elapsed: 0.01 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Train Loss: 188.4661, Train Accuracy: 4.79%, Test Loss: 4.3529, Test Accuracy: 12.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20], Train Loss: 185.9678, Train Accuracy: 46.80%, Test Loss: 3.6774, Test Accuracy: 39.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20], Train Loss: 184.1534, Train Accuracy: 70.57%, Test Loss: 3.2550, Test Accuracy: 48.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20], Train Loss: 182.6214, Train Accuracy: 81.06%, Test Loss: 2.9202, Test Accuracy: 57.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20], Train Loss: 181.2276, Train Accuracy: 85.68%, Test Loss: 2.6852, Test Accuracy: 63.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20], Train Loss: 179.9529, Train Accuracy: 86.69%, Test Loss: 2.5426, Test Accuracy: 66.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20], Train Loss: 178.7396, Train Accuracy: 87.01%, Test Loss: 2.3908, Test Accuracy: 68.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20], Train Loss: 177.5277, Train Accuracy: 87.56%, Test Loss: 2.3430, Test Accuracy: 70.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20], Train Loss: 176.3341, Train Accuracy: 87.84%, Test Loss: 2.2758, Test Accuracy: 69.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20], Train Loss: 175.1333, Train Accuracy: 87.92%, Test Loss: 2.1722, Test Accuracy: 74.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20], Train Loss: 173.9204, Train Accuracy: 88.14%, Test Loss: 2.1115, Test Accuracy: 74.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20], Train Loss: 172.7098, Train Accuracy: 88.22%, Test Loss: 2.1030, Test Accuracy: 72.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20], Train Loss: 171.5150, Train Accuracy: 88.24%, Test Loss: 2.0394, Test Accuracy: 73.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20], Train Loss: 170.3055, Train Accuracy: 88.22%, Test Loss: 2.0104, Test Accuracy: 73.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20], Train Loss: 169.0863, Train Accuracy: 88.20%, Test Loss: 1.9663, Test Accuracy: 74.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20], Train Loss: 167.8666, Train Accuracy: 88.28%, Test Loss: 1.9235, Test Accuracy: 75.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20], Train Loss: 166.6372, Train Accuracy: 88.31%, Test Loss: 1.8950, Test Accuracy: 76.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20], Train Loss: 165.4019, Train Accuracy: 88.26%, Test Loss: 1.8645, Test Accuracy: 76.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20], Train Loss: 164.1638, Train Accuracy: 88.26%, Test Loss: 1.8368, Test Accuracy: 76.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20], Train Loss: 162.9174, Train Accuracy: 88.31%, Test Loss: 1.8233, Test Accuracy: 76.02%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from kan import KANLayer  # Assuming kan is the module containing KANLayer\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from data_processors.pull_and_process_data import master_function\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Hyper-parameters\n",
    "batch_size = 128\n",
    "num_epochs = 20\n",
    "num_intervals = 8\n",
    "k = 6\n",
    "noise_scale = 0.1\n",
    "scale_base = 1.0\n",
    "scale_sp = 1.0\n",
    "base_fun = nn.SiLU()\n",
    "grid_eps = 0.0\n",
    "grid_range = [0, 1]\n",
    "sp_trainable = True\n",
    "sb_trainable = True\n",
    "# Sparsification parameters\n",
    "lambda_l1 = 1e-4\n",
    "lambda_entropy = 1e-4\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Pull and process data\n",
    "mouse_number = 715093703\n",
    "spike_df = master_function(session_number=mouse_number, output_dir=\"output\", timesteps_per_frame=1)\n",
    "spike_df = spike_df[spike_df['frame'] >= 0]\n",
    "\n",
    "# Prepare data\n",
    "X = spike_df.drop('frame', axis=1).values\n",
    "y = spike_df['frame'].values.astype(int)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "X = X_normalized\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.from_numpy(X_train).float().to(device)\n",
    "y_train = torch.from_numpy(y_train).long().to(device)\n",
    "X_test = torch.from_numpy(X_test).float().to(device)\n",
    "y_test = torch.from_numpy(y_test).long().to(device)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the KANLayer\n",
    "in_dim = X_train.shape[1]  # 2073 in this case\n",
    "out_dim = num_classes\n",
    "\n",
    "kan_layer = KANLayer(in_dim=in_dim, out_dim=out_dim, num=num_intervals, k=k, noise_scale=noise_scale, \n",
    "                     scale_base=scale_base, scale_sp=scale_sp, base_fun=base_fun, grid_eps=grid_eps,\n",
    "                     grid_range=grid_range, sp_trainable=sp_trainable, sb_trainable=sb_trainable, device=device).to(device)\n",
    "\n",
    "# Example forward pass\n",
    "def forward_pass(kan_layer, X):\n",
    "    y_pred, preacts, postacts, postspline = kan_layer(X)\n",
    "    return y_pred\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(kan_layer.parameters(), lr=0.001)\n",
    "\n",
    "# Helper function to compute sparsification penalties\n",
    "def compute_sparsification_penalty(kan_layer):\n",
    "    l1_penalty = 0.0\n",
    "    entropy_penalty = 0.0\n",
    "    for param in kan_layer.parameters():\n",
    "        l1_penalty += torch.sum(torch.abs(param))\n",
    "        param_normalized = torch.abs(param) / torch.sum(torch.abs(param))\n",
    "        entropy_penalty -= torch.sum(param_normalized * torch.log(param_normalized + 1e-10))\n",
    "    return lambda_l1 * l1_penalty + lambda_entropy * entropy_penalty\n",
    "\n",
    "# Training loop with sparsification\n",
    "for epoch in range(num_epochs):\n",
    "    kan_layer.train()\n",
    "    train_losses = []\n",
    "    train_correct = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    # Training phase\n",
    "    for X_batch, y_batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False):\n",
    "        # Forward pass\n",
    "        y_pred = forward_pass(kan_layer, X_batch)\n",
    "        # Compute loss\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        \n",
    "        # Compute sparsification penalty\n",
    "        sparsification_penalty = compute_sparsification_penalty(kan_layer)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = loss + sparsification_penalty\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(total_loss.item())\n",
    "        _, predicted = torch.max(y_pred.data, 1)\n",
    "        total_train += y_batch.size(0)\n",
    "        train_correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    train_accuracy = 100 * train_correct / total_train\n",
    "    \n",
    "    # Testing phase\n",
    "    kan_layer.eval()\n",
    "    test_losses = []\n",
    "    test_correct = 0\n",
    "    total_test = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            y_pred = forward_pass(kan_layer, X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            test_losses.append(loss.item())\n",
    "            _, predicted = torch.max(y_pred.data, 1)\n",
    "            total_test += y_batch.size(0)\n",
    "            test_correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    test_accuracy = 100 * test_correct / total_test\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {np.mean(train_losses):.4f}, Train Accuracy: {train_accuracy:.2f}%, Test Loss: {np.mean(test_losses):.4f}, Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8bc5d0ef-9d72-4a37-8e3d-6e564a5220d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated version 3!\n",
      "Initializing workflow...\n",
      "Loading existing datasets...\n",
      "Loaded spike trains dataset: <class 'pandas.core.frame.DataFrame'>\n",
      "Total time elapsed: 0.01 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/16], Train Loss: 188.0290, Train Accuracy: 6.93%, Test Loss: 4.2406, Test Accuracy: 19.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/16], Train Loss: 184.2855, Train Accuracy: 54.47%, Test Loss: 3.2804, Test Accuracy: 43.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/16], Train Loss: 181.4581, Train Accuracy: 77.78%, Test Loss: 2.8695, Test Accuracy: 57.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/16], Train Loss: 179.0327, Train Accuracy: 84.72%, Test Loss: 2.6087, Test Accuracy: 60.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/16], Train Loss: 176.7684, Train Accuracy: 86.69%, Test Loss: 2.3947, Test Accuracy: 66.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/16], Train Loss: 174.5394, Train Accuracy: 87.54%, Test Loss: 2.2611, Test Accuracy: 67.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/16], Train Loss: 172.3232, Train Accuracy: 88.05%, Test Loss: 2.1331, Test Accuracy: 71.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/16], Train Loss: 170.1048, Train Accuracy: 88.03%, Test Loss: 2.0538, Test Accuracy: 73.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/16], Train Loss: 167.8531, Train Accuracy: 88.22%, Test Loss: 2.0073, Test Accuracy: 73.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/16], Train Loss: 165.5976, Train Accuracy: 88.24%, Test Loss: 1.9333, Test Accuracy: 74.49%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/16], Train Loss: 163.3189, Train Accuracy: 88.20%, Test Loss: 1.8913, Test Accuracy: 74.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/16], Train Loss: 161.0274, Train Accuracy: 88.31%, Test Loss: 1.8483, Test Accuracy: 75.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/16], Train Loss: 158.7204, Train Accuracy: 88.24%, Test Loss: 1.8169, Test Accuracy: 76.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/16], Train Loss: 156.4054, Train Accuracy: 88.28%, Test Loss: 1.7938, Test Accuracy: 75.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/16], Train Loss: 154.6162, Train Accuracy: 88.31%, Test Loss: 1.7689, Test Accuracy: 75.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/16], Train Loss: 153.6548, Train Accuracy: 88.39%, Test Loss: 1.7530, Test Accuracy: 75.08%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from kan import KANLayer  # Assuming kan is the module containing KANLayer\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from data_processors.pull_and_process_data import master_function\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Hyper-parameters\n",
    "batch_size = 64\n",
    "num_epochs = 16\n",
    "num_intervals = 8\n",
    "k = 6\n",
    "noise_scale = 0.1\n",
    "scale_base = 1.0\n",
    "scale_sp = 1.0\n",
    "base_fun = nn.SiLU()\n",
    "grid_eps = 0.0\n",
    "grid_range = [0, 1]\n",
    "sp_trainable = True\n",
    "sb_trainable = True\n",
    "# Sparsification parameters\n",
    "lambda_l1 = 1e-4\n",
    "lambda_entropy = 1e-4\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Pull and process data\n",
    "mouse_number = 715093703\n",
    "spike_df = master_function(session_number=mouse_number, output_dir=\"output\", timesteps_per_frame=1)\n",
    "spike_df = spike_df[spike_df['frame'] >= 0]\n",
    "\n",
    "# Prepare data\n",
    "X = spike_df.drop('frame', axis=1).values\n",
    "y = spike_df['frame'].values.astype(int)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "X = X_normalized\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.from_numpy(X_train).float().to(device)\n",
    "y_train = torch.from_numpy(y_train).long().to(device)\n",
    "X_test = torch.from_numpy(X_test).float().to(device)\n",
    "y_test = torch.from_numpy(y_test).long().to(device)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the KANLayer\n",
    "in_dim = X_train.shape[1]  # 2073 in this case\n",
    "out_dim = num_classes\n",
    "\n",
    "kan_layer = KANLayer(in_dim=in_dim, out_dim=out_dim, num=num_intervals, k=k, noise_scale=noise_scale, \n",
    "                     scale_base=scale_base, scale_sp=scale_sp, base_fun=base_fun, grid_eps=grid_eps,\n",
    "                     grid_range=grid_range, sp_trainable=sp_trainable, sb_trainable=sb_trainable, device=device).to(device)\n",
    "\n",
    "# Example forward pass\n",
    "def forward_pass(kan_layer, X):\n",
    "    y_pred, preacts, postacts, postspline = kan_layer(X)\n",
    "    return y_pred\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(kan_layer.parameters(), lr=0.001)\n",
    "\n",
    "# Helper function to compute sparsification penalties\n",
    "def compute_sparsification_penalty(kan_layer):\n",
    "    l1_penalty = 0.0\n",
    "    entropy_penalty = 0.0\n",
    "    for param in kan_layer.parameters():\n",
    "        l1_penalty += torch.sum(torch.abs(param))\n",
    "        param_normalized = torch.abs(param) / torch.sum(torch.abs(param))\n",
    "        entropy_penalty -= torch.sum(param_normalized * torch.log(param_normalized + 1e-10))\n",
    "    return lambda_l1 * l1_penalty + lambda_entropy * entropy_penalty\n",
    "\n",
    "# Training loop with sparsification\n",
    "for epoch in range(num_epochs):\n",
    "    kan_layer.train()\n",
    "    train_losses = []\n",
    "    train_correct = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    # Training phase\n",
    "    for X_batch, y_batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False):\n",
    "        # Forward pass\n",
    "        y_pred = forward_pass(kan_layer, X_batch)\n",
    "        # Compute loss\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        \n",
    "        # Compute sparsification penalty\n",
    "        sparsification_penalty = compute_sparsification_penalty(kan_layer)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = loss + sparsification_penalty\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(total_loss.item())\n",
    "        _, predicted = torch.max(y_pred.data, 1)\n",
    "        total_train += y_batch.size(0)\n",
    "        train_correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    train_accuracy = 100 * train_correct / total_train\n",
    "    \n",
    "    # Testing phase\n",
    "    kan_layer.eval()\n",
    "    test_losses = []\n",
    "    test_correct = 0\n",
    "    total_test = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            y_pred = forward_pass(kan_layer, X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            test_losses.append(loss.item())\n",
    "            _, predicted = torch.max(y_pred.data, 1)\n",
    "            total_test += y_batch.size(0)\n",
    "            test_correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    test_accuracy = 100 * test_correct / total_test\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {np.mean(train_losses):.4f}, Train Accuracy: {train_accuracy:.2f}%, Test Loss: {np.mean(test_losses):.4f}, Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac4dd77-5a33-41c1-af76-6f57b36fcbeb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### KAN with PCA Layer as Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50418f58-3948-44da-bdb6-09c9f314e9db",
   "metadata": {},
   "source": [
    "This works much quicker than running each neuron as an input, but at the cost of reducing accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c16fcb89-0c1e-46f0-8f43-57228364b951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated version 3!\n",
      "Initializing workflow...\n",
      "Loading existing datasets...\n",
      "Loaded spike trains dataset: <class 'pandas.core.frame.DataFrame'>\n",
      "Total time elapsed: 0.01 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Train Loss: 10.2513, Train Accuracy: 37.75%, Test Loss: 2.1236, Test Accuracy: 59.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Train Loss: 7.8138, Train Accuracy: 74.89%, Test Loss: 1.8550, Test Accuracy: 62.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Train Loss: 7.0677, Train Accuracy: 79.75%, Test Loss: 1.8371, Test Accuracy: 61.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Train Loss: 6.7355, Train Accuracy: 81.10%, Test Loss: 1.8331, Test Accuracy: 60.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Train Loss: 6.5631, Train Accuracy: 82.06%, Test Loss: 1.8799, Test Accuracy: 59.24%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from kan import KANLayer  \n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from data_processors.pull_and_process_data import master_function\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Hyper-parameters\n",
    "batch_size = 16\n",
    "num_epochs = 5\n",
    "num_intervals = 3\n",
    "k = 3\n",
    "noise_scale = 0.1\n",
    "scale_base = 1.0\n",
    "scale_sp = 1.0\n",
    "base_fun = nn.SiLU()\n",
    "grid_eps = 0.0\n",
    "grid_range = [0, 1]\n",
    "sp_trainable = True\n",
    "sb_trainable = True\n",
    "n_components = 120\n",
    "# Sparsification parameters\n",
    "lambda_l1 = 1e-4\n",
    "lambda_entropy = 1e-4\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Pull and process data\n",
    "mouse_number = 715093703\n",
    "spike_df = master_function(session_number=mouse_number, output_dir=\"output\", timesteps_per_frame=1)\n",
    "spike_df = spike_df[spike_df['frame'] >= 0]\n",
    "\n",
    "# Prepare data\n",
    "X = spike_df.drop('frame', axis=1).values\n",
    "y = spike_df['frame'].values.astype(int)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "X = X_normalized\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Perform PCA to reduce dimensions\n",
    "pca = PCA(n_components=n_components)\n",
    "X_train_reduced = pca.fit_transform(X_train)\n",
    "X_test_reduced = pca.transform(X_test)\n",
    "\n",
    "# Convert reduced data back to PyTorch tensors\n",
    "X_train_reduced = torch.from_numpy(X_train_reduced).float().to(device)\n",
    "X_test_reduced = torch.from_numpy(X_test_reduced).float().to(device)\n",
    "y_train = torch.from_numpy(y_train).long().to(device)\n",
    "y_test = torch.from_numpy(y_test).long().to(device)\n",
    "\n",
    "# Create DataLoader with reduced data\n",
    "train_dataset_reduced = TensorDataset(X_train_reduced, y_train)\n",
    "test_dataset_reduced = TensorDataset(X_test_reduced, y_test)\n",
    "train_loader_reduced = DataLoader(train_dataset_reduced, batch_size=batch_size, shuffle=True)\n",
    "test_loader_reduced = DataLoader(test_dataset_reduced, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the KANLayer with reduced input dimension\n",
    "in_dim_reduced = X_train_reduced.shape[1]  # This should be equal to n_components\n",
    "out_dim = num_classes\n",
    "\n",
    "kan_layer_reduced = KANLayer(in_dim=in_dim_reduced, out_dim=out_dim, num=num_intervals, k=k, noise_scale=noise_scale, \n",
    "                             scale_base=scale_base, scale_sp=scale_sp, base_fun=base_fun, grid_eps=grid_eps,\n",
    "                             grid_range=grid_range, sp_trainable=sp_trainable, sb_trainable=sb_trainable, device=device).to(device)\n",
    "\n",
    "# Example forward pass\n",
    "def forward_pass(kan_layer, X):\n",
    "    y_pred, preacts, postacts, postspline = kan_layer(X)\n",
    "    return y_pred\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(kan_layer_reduced.parameters(), lr=0.01)\n",
    "\n",
    "# Helper function to compute sparsification penalties\n",
    "def compute_sparsification_penalty(kan_layer):\n",
    "    l1_penalty = 0.0\n",
    "    entropy_penalty = 0.0\n",
    "    for param in kan_layer.parameters():\n",
    "        l1_penalty += torch.sum(torch.abs(param))\n",
    "        param_normalized = torch.abs(param) / torch.sum(torch.abs(param))\n",
    "        entropy_penalty -= torch.sum(param_normalized * torch.log(param_normalized + 1e-10))\n",
    "    return lambda_l1 * l1_penalty + lambda_entropy * entropy_penalty\n",
    "\n",
    "# Training loop with reduced data\n",
    "for epoch in range(num_epochs):\n",
    "    kan_layer_reduced.train()\n",
    "    train_losses = []\n",
    "    train_correct = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    # Training phase\n",
    "    for X_batch, y_batch in tqdm(train_loader_reduced, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False):\n",
    "        # Forward pass\n",
    "        y_pred = forward_pass(kan_layer_reduced, X_batch)\n",
    "        # Compute loss\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        \n",
    "        # Compute sparsification penalty\n",
    "        sparsification_penalty = compute_sparsification_penalty(kan_layer_reduced)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = loss + sparsification_penalty\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(total_loss.item())\n",
    "        _, predicted = torch.max(y_pred.data, 1)\n",
    "        total_train += y_batch.size(0)\n",
    "        train_correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    train_accuracy = 100 * train_correct / total_train\n",
    "    \n",
    "    # Testing phase\n",
    "    kan_layer_reduced.eval()\n",
    "    test_losses = []\n",
    "    test_correct = 0\n",
    "    total_test = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader_reduced:\n",
    "            y_pred = forward_pass(kan_layer_reduced, X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            test_losses.append(loss.item())\n",
    "            _, predicted = torch.max(y_pred.data, 1)\n",
    "            total_test += y_batch.size(0)\n",
    "            test_correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    test_accuracy = 100 * test_correct / total_test\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {np.mean(train_losses):.4f}, Train Accuracy: {train_accuracy:.2f}%, Test Loss: {np.mean(test_losses):.4f}, Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5128698c-693e-4f48-ab79-e84f13b6f123",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### KAN Symbolic layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c42d05fc-8d52-48f7-81d2-5e5a2f80774c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated version 3!\n",
      "Initializing workflow...\n",
      "Loading existing datasets...\n",
      "Loaded spike trains dataset: <class 'pandas.core.frame.DataFrame'>\n",
      "Total time elapsed: 0.01 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Train Loss: nan, Train Accuracy: 0.85%, Test Loss: 4.7707, Test Accuracy: 0.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Train Loss: nan, Train Accuracy: 0.85%, Test Loss: 4.7707, Test Accuracy: 0.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Train Loss: nan, Train Accuracy: 0.85%, Test Loss: 4.7707, Test Accuracy: 0.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Train Loss: nan, Train Accuracy: 0.85%, Test Loss: 4.7707, Test Accuracy: 0.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Train Loss: nan, Train Accuracy: 0.85%, Test Loss: 4.7707, Test Accuracy: 0.85%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from kan import Symbolic_KANLayer  \n",
    "import numpy as np\n",
    "from data_processors.pull_and_process_data import master_function\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from data_processors.pull_and_process_data import master_function\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Hyper-parameters\n",
    "batch_size = 128\n",
    "num_epochs = 5\n",
    "num_intervals = 4\n",
    "k = 3\n",
    "noise_scale = 0.1\n",
    "scale_base = 1.0\n",
    "scale_sp = 1.0\n",
    "base_fun = nn.SiLU()\n",
    "grid_eps = 0.01\n",
    "grid_range = [0, 1]\n",
    "sp_trainable = True\n",
    "sb_trainable = True\n",
    "n_components = 12\n",
    "# Sparsification parameters\n",
    "lambda_l1 = 1e-4\n",
    "lambda_entropy = 1e-4\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Pull and process data\n",
    "mouse_number = 715093703\n",
    "spike_df = master_function(session_number=mouse_number, output_dir=\"output\", timesteps_per_frame=1)\n",
    "spike_df = spike_df[spike_df['frame'] >= 0]\n",
    "\n",
    "# Prepare data\n",
    "X = spike_df.drop('frame', axis=1).values\n",
    "y = spike_df['frame'].values.astype(int)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "X = X_normalized\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Perform PCA to reduce dimensions\n",
    "pca = PCA(n_components=n_components)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.from_numpy(X_train).float().to(device)\n",
    "X_test = torch.from_numpy(X_test).float().to(device)\n",
    "y_train = torch.from_numpy(y_train).long().to(device)\n",
    "y_test = torch.from_numpy(y_test).long().to(device)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the Symbolic_KANLayer\n",
    "in_dim = X_train.shape[1]  # 2073 in this case\n",
    "out_dim = num_classes\n",
    "\n",
    "symbolic_kan_layer = Symbolic_KANLayer(in_dim=n_components, out_dim=out_dim, device=device).to(device)\n",
    "\n",
    "# Define symbolic functions for the layer (example, you can modify as needed)\n",
    "for i in range(in_dim):\n",
    "    for j in range(out_dim):\n",
    "        symbolic_kan_layer.fix_symbolic(i, j, 'sin')  # Replace 'sin' with the desired symbolic function\n",
    "\n",
    "# Example forward pass\n",
    "def forward_pass(symbolic_kan_layer, X):\n",
    "    y_pred, postacts = symbolic_kan_layer(X)\n",
    "    return y_pred\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(symbolic_kan_layer.parameters(), lr=0.01)\n",
    "\n",
    "# Helper function to compute sparsification penalties\n",
    "def compute_sparsification_penalty(symbolic_kan_layer):\n",
    "    l1_penalty = 0.0\n",
    "    entropy_penalty = 0.0\n",
    "    for param in symbolic_kan_layer.parameters():\n",
    "        l1_penalty += torch.sum(torch.abs(param))\n",
    "        param_normalized = torch.abs(param) / torch.sum(torch.abs(param))\n",
    "        entropy_penalty -= torch.sum(param_normalized * torch.log(param_normalized + 1e-10))\n",
    "    return lambda_l1 * l1_penalty + lambda_entropy * entropy_penalty\n",
    "\n",
    "# Training loop with sparsification\n",
    "for epoch in range(num_epochs):\n",
    "    symbolic_kan_layer.train()\n",
    "    train_losses = []\n",
    "    train_correct = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    # Training phase\n",
    "    for X_batch, y_batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False):\n",
    "        # Forward pass\n",
    "        y_pred = forward_pass(symbolic_kan_layer, X_batch)\n",
    "        # Compute loss\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        \n",
    "        # Compute sparsification penalty\n",
    "        sparsification_penalty = compute_sparsification_penalty(symbolic_kan_layer)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = loss + sparsification_penalty\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(total_loss.item())\n",
    "        _, predicted = torch.max(y_pred.data, 1)\n",
    "        total_train += y_batch.size(0)\n",
    "        train_correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    train_accuracy = 100 * train_correct / total_train\n",
    "    \n",
    "    # Testing phase\n",
    "    symbolic_kan_layer.eval()\n",
    "    test_losses = []\n",
    "    test_correct = 0\n",
    "    total_test = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            y_pred = forward_pass(symbolic_kan_layer, X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            test_losses.append(loss.item())\n",
    "            _, predicted = torch.max(y_pred.data, 1)\n",
    "            total_test += y_batch.size(0)\n",
    "            test_correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    test_accuracy = 100 * test_correct / total_test\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {np.mean(train_losses):.4f}, Train Accuracy: {train_accuracy:.2f}%, Test Loss: {np.mean(test_losses):.4f}, Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1622a8ff-ac21-45e3-b597-757181a2109b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Multi-Layered KAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffcd7f9-d537-4021-b524-fc28be9913d4",
   "metadata": {},
   "source": [
    "I may come back and redo this section in the future as it is very computationally expensive in it's current iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6e15a6e-2c47-4bd0-ba87-24fc2ffc6ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated version 3!\n",
      "Initializing workflow...\n",
      "Loading existing datasets...\n",
      "Loaded spike trains dataset: <class 'pandas.core.frame.DataFrame'>\n",
      "Total time elapsed: 0.01 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.35 GiB. GPU 0 has a total capacity of 9.62 GiB of which 2.10 GiB is free. Including non-PyTorch memory, this process has 7.49 GiB memory in use. Of the allocated memory 6.49 GiB is allocated by PyTorch, and 866.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2142558/3690821441.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'Epoch {epoch+1}/{num_epochs}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkan_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# Compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2142558/3690821441.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0my_pred1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreacts1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostacts1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostspline1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkan_layer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0my_pred2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreacts2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostacts2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostspline2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkan_layer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my_pred2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/kan/KANLayer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mpreacts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mbase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shape (batch, size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoef2curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_sharing\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_sharing\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shape (size, batch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shape (batch, size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mpostspline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/kan/spline.py\u001b[0m in \u001b[0;36mcoef2curve\u001b[0;34m(x_eval, grid, coef, k, device)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcoef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mx_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mcoef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0my_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ij,ijk->ik'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my_eval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/kan/spline.py\u001b[0m in \u001b[0;36mB_batch\u001b[0;34m(x, grid, k, extend, device)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mB_km1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mB_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mB_km1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mB_km1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/kan/spline.py\u001b[0m in \u001b[0;36mB_batch\u001b[0;34m(x, grid, k, extend, device)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mB_km1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mB_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mB_km1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mB_km1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/kan/spline.py\u001b[0m in \u001b[0;36mB_batch\u001b[0;34m(x, grid, k, extend, device)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mB_km1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mB_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mB_km1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mB_km1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/kan/spline.py\u001b[0m in \u001b[0;36mB_batch\u001b[0;34m(x, grid, k, extend, device)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mB_km1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mB_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mB_km1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mB_km1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/kan/spline.py\u001b[0m in \u001b[0;36mB_batch\u001b[0;34m(x, grid, k, extend, device)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mB_km1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mB_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mB_km1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mB_km1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/kan/spline.py\u001b[0m in \u001b[0;36mB_batch\u001b[0;34m(x, grid, k, extend, device)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mB_km1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mB_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mB_km1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mB_km1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.35 GiB. GPU 0 has a total capacity of 9.62 GiB of which 2.10 GiB is free. Including non-PyTorch memory, this process has 7.49 GiB memory in use. Of the allocated memory 6.49 GiB is allocated by PyTorch, and 866.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from kan import KANLayer  # Assuming kan is the module containing KANLayer\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from data_processors.pull_and_process_data import master_function\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Pull and process data\n",
    "mouse_number = 715093703\n",
    "spike_df = master_function(session_number=mouse_number, output_dir=\"output\", timesteps_per_frame=1)\n",
    "spike_df = spike_df[spike_df['frame'] >= 0]\n",
    "\n",
    "# Prepare data\n",
    "X = spike_df.drop('frame', axis=1).values\n",
    "y = spike_df['frame'].values.astype(int)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "X = X_normalized\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.from_numpy(X_train).float().to(device)\n",
    "y_train = torch.from_numpy(y_train).long().to(device)\n",
    "X_test = torch.from_numpy(X_test).float().to(device)\n",
    "y_test = torch.from_numpy(y_test).long().to(device)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 16\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define a custom KAN model with an intermediate layer\n",
    "class CustomKANModel(nn.Module):\n",
    "    def __init__(self, in_dim, inter_dim, out_dim, num_intervals, k, noise_scale, scale_base, scale_sp, base_fun, grid_eps, grid_range, sp_trainable, sb_trainable, device):\n",
    "        super(CustomKANModel, self).__init__()\n",
    "        self.kan_layer1 = KANLayer(in_dim=in_dim, out_dim=inter_dim, num=num_intervals, k=k, noise_scale=noise_scale, \n",
    "                                   scale_base=scale_base, scale_sp=scale_sp, base_fun=base_fun, grid_eps=grid_eps,\n",
    "                                   grid_range=grid_range, sp_trainable=sp_trainable, sb_trainable=sb_trainable, device=device).to(device)\n",
    "        self.kan_layer2 = KANLayer(in_dim=inter_dim, out_dim=out_dim, num=num_intervals, k=k, noise_scale=noise_scale, \n",
    "                                   scale_base=scale_base, scale_sp=scale_sp, base_fun=base_fun, grid_eps=grid_eps,\n",
    "                                   grid_range=grid_range, sp_trainable=sp_trainable, sb_trainable=sb_trainable, device=device).to(device)\n",
    "\n",
    "    def forward(self, X):\n",
    "        y_pred1, preacts1, postacts1, postspline1 = self.kan_layer1(X)\n",
    "        y_pred2, preacts2, postacts2, postspline2 = self.kan_layer2(y_pred1)\n",
    "        return y_pred2\n",
    "\n",
    "# Initialize the CustomKANModel\n",
    "in_dim = X_train.shape[1]  # 2073 in this case\n",
    "inter_dim = 1000  # Example intermediate dimension\n",
    "out_dim = num_classes\n",
    "num_intervals = 8\n",
    "k = 6\n",
    "noise_scale = 0.1\n",
    "scale_base = 1.0\n",
    "scale_sp = 1.0\n",
    "base_fun = nn.SiLU()\n",
    "grid_eps = 1\n",
    "grid_range = [0, 1]\n",
    "sp_trainable = True\n",
    "sb_trainable = True\n",
    "\n",
    "kan_model = CustomKANModel(in_dim=in_dim, inter_dim=inter_dim, out_dim=out_dim, num_intervals=num_intervals, k=k, noise_scale=noise_scale, \n",
    "                            scale_base=scale_base, scale_sp=scale_sp, base_fun=base_fun, grid_eps=grid_eps, grid_range=grid_range, \n",
    "                            sp_trainable=sp_trainable, sb_trainable=sb_trainable, device=device).to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(kan_model.parameters(), lr=0.01)\n",
    "\n",
    "# Sparsification parameters\n",
    "lambda_l1 = 1e-4\n",
    "lambda_entropy = 1e-4\n",
    "\n",
    "# Helper function to compute sparsification penalties\n",
    "def compute_sparsification_penalty(model):\n",
    "    l1_penalty = 0.0\n",
    "    entropy_penalty = 0.0\n",
    "    for param in model.parameters():\n",
    "        l1_penalty += torch.sum(torch.abs(param))\n",
    "        param_normalized = torch.abs(param) / torch.sum(torch.abs(param))\n",
    "        entropy_penalty -= torch.sum(param_normalized * torch.log(param_normalized + 1e-10))\n",
    "    return lambda_l1 * l1_penalty + lambda_entropy * entropy_penalty\n",
    "\n",
    "# Training loop with sparsification\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    kan_model.train()\n",
    "    train_losses = []\n",
    "    train_correct = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    # Training phase\n",
    "    for X_batch, y_batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False):\n",
    "        # Forward pass\n",
    "        y_pred = kan_model(X_batch)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        \n",
    "        # Compute sparsification penalty\n",
    "        sparsification_penalty = compute_sparsification_penalty(kan_model)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = loss + sparsification_penalty\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(total_loss.item())\n",
    "        _, predicted = torch.max(y_pred.data, 1)\n",
    "        total_train += y_batch.size(0)\n",
    "        train_correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    train_accuracy = 100 * train_correct / total_train\n",
    "    \n",
    "    # Testing phase\n",
    "    kan_model.eval()\n",
    "    test_losses = []\n",
    "    test_correct = 0\n",
    "    total_test = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            y_pred = kan_model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            test_losses.append(loss.item())\n",
    "            _, predicted = torch.max(y_pred.data, 1)\n",
    "            total_test += y_batch.size(0)\n",
    "            test_correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    test_accuracy = 100 * test_correct / total_test\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {np.mean(train_losses):.4f}, Train Accuracy: {train_accuracy:.2f}%, Test Loss: {np.mean(test_losses):.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "# Pruning function\n",
    "def prune_kan_layer(kan_layer, threshold=1e-2):\n",
    "    with torch.no_grad():\n",
    "        for name, param in kan_layer.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                param *= (torch.abs(param) > threshold).float()\n",
    "\n",
    "# Prune the KAN layers after training\n",
    "prune_kan_layer(kan_model.kan_layer1)\n",
    "prune_kan_layer(kan_model.kan_layer2)\n",
    "\n",
    "# Retrain the pruned KAN model\n",
    "for epoch in range(num_epochs):\n",
    "    kan_model.train()\n",
    "    train_losses = []\n",
    "    train_correct = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    # Training phase\n",
    "    for X_batch, y_batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False):\n",
    "        # Forward pass\n",
    "        y_pred = kan_model(X_batch)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        \n",
    "        # Compute sparsification penalty\n",
    "        sparsification_penalty = compute_sparsification_penalty(kan_model)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = loss + sparsification_penalty\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(total_loss.item())\n",
    "        _, predicted = torch.max(y_pred.data, 1)\n",
    "        total_train += y_batch.size(0)\n",
    "        train_correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    train_accuracy = 100 * train_correct / total_train\n",
    "    \n",
    "    # Testing phase\n",
    "    kan_model.eval()\n",
    "    test_losses = []\n",
    "    test_correct = 0\n",
    "    total_test = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            y_pred = kan_model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            test_losses.append(loss.item())\n",
    "            _, predicted = torch.max(y_pred.data, 1)\n",
    "            total_test += y_batch.size(0)\n",
    "            test_correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    test_accuracy = 100 * test_correct / total_test\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {np.mean(train_losses):.4f}, Train Accuracy: {train_accuracy:.2f}%, Test Loss: {np.mean(test_losses):.4f}, Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c742c10-7d0e-4cf9-a0d3-344c46571868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated version 3!\n",
      "Initializing workflow...\n",
      "Loading existing datasets...\n",
      "Loaded spike trains dataset: <class 'pandas.core.frame.DataFrame'>\n",
      "Total time elapsed: 0.01 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/6], Train Loss: 739.1743, Train Accuracy: 0.68%, Test Loss: 4.8930, Test Accuracy: 1.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/6], Train Loss: 573.5616, Train Accuracy: 1.50%, Test Loss: 4.7925, Test Accuracy: 3.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/6], Train Loss: 560.4291, Train Accuracy: 11.29%, Test Loss: 3.5963, Test Accuracy: 19.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/6], Train Loss: 558.4958, Train Accuracy: 38.33%, Test Loss: 2.2982, Test Accuracy: 47.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/6], Train Loss: 557.6904, Train Accuracy: 59.11%, Test Loss: 1.8608, Test Accuracy: 57.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/6], Train Loss: 557.2176, Train Accuracy: 68.94%, Test Loss: 1.8196, Test Accuracy: 56.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/6], Train Loss: 557.0008, Train Accuracy: 74.64%, Test Loss: 1.6900, Test Accuracy: 57.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/6], Train Loss: 556.9613, Train Accuracy: 76.97%, Test Loss: 1.6114, Test Accuracy: 60.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/6], Train Loss: 557.0994, Train Accuracy: 75.47%, Test Loss: 2.3321, Test Accuracy: 49.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_77036/3420617986.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_kan_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprune_and_retrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_77036/3420617986.py\u001b[0m in \u001b[0;36mprune_and_retrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'Epoch {epoch+1}/{self.num_epochs}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                 \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkan_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0msparsification_penalty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_sparsification_penalty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkan_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_77036/3420617986.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m                     \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m                     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/kan/KANLayer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mpreacts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mbase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shape (batch, size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoef2curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_sharing\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_sharing\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shape (size, batch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shape (batch, size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mpostspline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from kan import KANLayer  # Assuming kan is the module containing KANLayer\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from data_processors.pull_and_process_data import master_function\n",
    "from tqdm import tqdm\n",
    "\n",
    "class KANModelTrainer:\n",
    "    def __init__(self, mouse_number, output_dir, timesteps_per_frame, batch_size=16, num_epochs=100, device=None):\n",
    "        self.mouse_number = mouse_number\n",
    "        self.output_dir = output_dir\n",
    "        self.timesteps_per_frame = timesteps_per_frame\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.lambda_l1 = 1e-4\n",
    "        self.lambda_entropy = 1e-4\n",
    "\n",
    "        # Set default hyperparameters\n",
    "        self.in_dim = None\n",
    "        self.inter_dim = 250\n",
    "        self.out_dim = None\n",
    "        self.num_intervals = 7\n",
    "        self.k = 6\n",
    "        self.noise_scale = 0.1\n",
    "        self.scale_base = 1.0\n",
    "        self.scale_sp = 1.0\n",
    "        self.base_fun = nn.SiLU()\n",
    "        self.grid_eps = 0.02\n",
    "        self.grid_range = [0, 1]\n",
    "        self.sp_trainable = True\n",
    "        self.sb_trainable = True\n",
    "        self.num_layers = 3\n",
    "        self.lr = 0.01\n",
    "\n",
    "    def set_hyperparameters(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        spike_df = master_function(session_number=self.mouse_number, output_dir=self.output_dir, timesteps_per_frame=self.timesteps_per_frame)\n",
    "        spike_df = spike_df[spike_df['frame'] >= 0]\n",
    "\n",
    "        X = spike_df.drop('frame', axis=1).values\n",
    "        y = spike_df['frame'].values.astype(int)\n",
    "\n",
    "        scaler = MinMaxScaler()\n",
    "        X_normalized = scaler.fit_transform(X)\n",
    "        X = X_normalized\n",
    "        self.out_dim = len(np.unique(y))\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "        \n",
    "        self.in_dim = X_train.shape[1]\n",
    "\n",
    "        X_train = torch.from_numpy(X_train).float().to(self.device)\n",
    "        y_train = torch.from_numpy(y_train).long().to(self.device)\n",
    "        X_test = torch.from_numpy(X_test).float().to(self.device)\n",
    "        y_test = torch.from_numpy(y_test).long().to(self.device)\n",
    "\n",
    "        train_dataset = TensorDataset(X_train, y_train)\n",
    "        test_dataset = TensorDataset(X_test, y_test)\n",
    "        self.train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        self.test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "    def create_kan_model(self):\n",
    "        class CustomKANModel(nn.Module):\n",
    "            def __init__(self, in_dim, inter_dim, out_dim, num_intervals, k, noise_scale, scale_base, scale_sp, base_fun, grid_eps, grid_range, sp_trainable, sb_trainable, num_layers, device):\n",
    "                super(CustomKANModel, self).__init__()\n",
    "                self.layers = nn.ModuleList()\n",
    "                self.layers.append(KANLayer(in_dim=in_dim, out_dim=inter_dim, num=num_intervals, k=k, noise_scale=noise_scale, \n",
    "                                            scale_base=scale_base, scale_sp=scale_sp, base_fun=base_fun, grid_eps=grid_eps,\n",
    "                                            grid_range=grid_range, sp_trainable=sp_trainable, sb_trainable=sb_trainable, device=device).to(device))\n",
    "                for _ in range(num_layers - 2):\n",
    "                    self.layers.append(KANLayer(in_dim=inter_dim, out_dim=inter_dim, num=num_intervals, k=k, noise_scale=noise_scale, \n",
    "                                                scale_base=scale_base, scale_sp=scale_sp, base_fun=base_fun, grid_eps=grid_eps,\n",
    "                                                grid_range=grid_range, sp_trainable=sp_trainable, sb_trainable=sb_trainable, device=device).to(device))\n",
    "                self.layers.append(KANLayer(in_dim=inter_dim, out_dim=out_dim, num=num_intervals, k=k, noise_scale=noise_scale, \n",
    "                                            scale_base=scale_base, scale_sp=scale_sp, base_fun=base_fun, grid_eps=grid_eps,\n",
    "                                            grid_range=grid_range, sp_trainable=sp_trainable, sb_trainable=sb_trainable, device=device).to(device))\n",
    "            \n",
    "            def forward(self, X):\n",
    "                for layer in self.layers:\n",
    "                    y_pred, _, _, _ = layer(X)\n",
    "                    X = y_pred\n",
    "                return y_pred\n",
    "\n",
    "        self.kan_model = CustomKANModel(self.in_dim, self.inter_dim, self.out_dim, self.num_intervals, self.k, self.noise_scale, \n",
    "                                        self.scale_base, self.scale_sp, self.base_fun, self.grid_eps, self.grid_range, \n",
    "                                        self.sp_trainable, self.sb_trainable, self.num_layers, self.device).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.kan_model.parameters(), lr=self.lr)\n",
    "\n",
    "    def compute_sparsification_penalty(self, model):\n",
    "        l1_penalty = 0.0\n",
    "        entropy_penalty = 0.0\n",
    "        for param in model.parameters():\n",
    "            l1_penalty += torch.sum(torch.abs(param))\n",
    "            param_normalized = torch.abs(param) / torch.sum(torch.abs(param))\n",
    "            entropy_penalty -= torch.sum(param_normalized * torch.log(param_normalized + 1e-10))\n",
    "        return self.lambda_l1 * l1_penalty + self.lambda_entropy * entropy_penalty\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.kan_model.train()\n",
    "            train_losses = []\n",
    "            train_correct = 0\n",
    "            total_train = 0\n",
    "            \n",
    "            for X_batch, y_batch in tqdm(self.train_loader, desc=f'Epoch {epoch+1}/{self.num_epochs}', leave=False):\n",
    "                y_pred = self.kan_model(X_batch)\n",
    "                loss = self.criterion(y_pred, y_batch)\n",
    "                sparsification_penalty = self.compute_sparsification_penalty(self.kan_model)\n",
    "                total_loss = loss + sparsification_penalty\n",
    "                self.optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                train_losses.append(total_loss.item())\n",
    "                _, predicted = torch.max(y_pred.data, 1)\n",
    "                total_train += y_batch.size(0)\n",
    "                train_correct += (predicted == y_batch).sum().item()\n",
    "            \n",
    "            train_accuracy = 100 * train_correct / total_train\n",
    "            \n",
    "            self.kan_model.eval()\n",
    "            test_losses = []\n",
    "            test_correct = 0\n",
    "            total_test = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for X_batch, y_batch in self.test_loader:\n",
    "                    y_pred = self.kan_model(X_batch)\n",
    "                    loss = self.criterion(y_pred, y_batch)\n",
    "                    test_losses.append(loss.item())\n",
    "                    _, predicted = torch.max(y_pred.data, 1)\n",
    "                    total_test += y_batch.size(0)\n",
    "                    test_correct += (predicted == y_batch).sum().item()\n",
    "            \n",
    "            test_accuracy = 100 * test_correct / total_test\n",
    "            \n",
    "            print(f'Epoch [{epoch+1}/{self.num_epochs}], Train Loss: {np.mean(train_losses):.4f}, Train Accuracy: {train_accuracy:.2f}%, Test Loss: {np.mean(test_losses):.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "    def prune_kan_layer(self, kan_layer, threshold=1e-2):\n",
    "        with torch.no_grad():\n",
    "            for name, param in kan_layer.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    param *= (torch.abs(param) > threshold).float()\n",
    "\n",
    "    def prune_and_retrain(self):\n",
    "        for layer in self.kan_model.layers:\n",
    "            self.prune_kan_layer(layer)\n",
    "        \n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.kan_model.train()\n",
    "            train_losses = []\n",
    "            train_correct = 0\n",
    "            total_train = 0\n",
    "            \n",
    "            for X_batch, y_batch in tqdm(self.train_loader, desc=f'Epoch {epoch+1}/{self.num_epochs}', leave=False):\n",
    "                y_pred = self.kan_model(X_batch)\n",
    "                loss = self.criterion(y_pred, y_batch)\n",
    "                sparsification_penalty = self.compute_sparsification_penalty(self.kan_model)\n",
    "                total_loss = loss + sparsification_penalty\n",
    "                self.optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                train_losses.append(total_loss.item())\n",
    "                _, predicted = torch.max(y_pred.data, 1)\n",
    "                total_train += y_batch.size(0)\n",
    "                train_correct += (predicted == y_batch).sum().item()\n",
    "            \n",
    "            train_accuracy = 100 * train_correct / total_train\n",
    "            \n",
    "            self.kan_model.eval()\n",
    "            test_losses = []\n",
    "            test_correct = 0\n",
    "            total_test = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for X_batch, y_batch in self.test_loader:\n",
    "                    y_pred = self.kan_model(X_batch)\n",
    "                    loss = self.criterion(y_pred, y_batch)\n",
    "                    test_losses.append(loss.item())\n",
    "                    _, predicted = torch.max(y_pred.data, 1)\n",
    "                    total_test += y_batch.size(0)\n",
    "                    test_correct += (predicted == y_batch).sum().item()\n",
    "            \n",
    "            test_accuracy = 100 * test_correct / total_test\n",
    "            \n",
    "            print(f'Epoch [{epoch+1}/{self.num_epochs}], Train Loss: {np.mean(train_losses):.4f}, Train Accuracy: {train_accuracy:.2f}%, Test Loss: {np.mean(test_losses):.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "# Example usage\n",
    "trainer = KANModelTrainer(mouse_number=715093703, output_dir=\"output\", timesteps_per_frame=1, batch_size=16, num_epochs=6)\n",
    "trainer.set_hyperparameters(inter_dim=500, num_layers=1, lr=0.01)\n",
    "trainer.prepare_data()\n",
    "trainer.create_kan_model()\n",
    "trainer.train()\n",
    "trainer.prune_and_retrain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cec518a-b22d-4ba4-b475-6dbae6b83ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from kan import KANLayer  # Assuming kan is the module containing KANLayer\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from data_processors.pull_and_process_data import master_function\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Pull and process data\n",
    "mouse_number = 715093703\n",
    "spike_df = master_function(session_number=mouse_number, output_dir=\"output\", timesteps_per_frame=1)\n",
    "spike_df = spike_df[spike_df['frame'] >= 0]\n",
    "\n",
    "# Prepare data\n",
    "X = spike_df.drop('frame', axis=1).values\n",
    "y = spike_df['frame'].values.astype(int)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "X = X_normalized\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.from_numpy(X_train).float().to(device)\n",
    "y_train = torch.from_numpy(y_train).long().to(device)\n",
    "X_test = torch.from_numpy(X_test).float().to(device)\n",
    "y_test = torch.from_numpy(y_test).long().to(device)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 16\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define a custom KAN model with an intermediate layer\n",
    "class CustomKANModel(nn.Module):\n",
    "    def __init__(self, in_dim, inter_dim, out_dim, num_intervals, k, noise_scale, scale_base, scale_sp, base_fun, grid_eps, grid_range, sp_trainable, sb_trainable, device):\n",
    "        super(CustomKANModel, self).__init__()\n",
    "        self.kan_layer1 = KANLayer(in_dim=in_dim, out_dim=inter_dim, num=num_intervals, k=k, noise_scale=noise_scale, \n",
    "                                   scale_base=scale_base, scale_sp=scale_sp, base_fun=base_fun, grid_eps=grid_eps,\n",
    "                                   grid_range=grid_range, sp_trainable=sp_trainable, sb_trainable=sb_trainable, device=device).to(device)\n",
    "        self.kan_layer2 = KANLayer(in_dim=inter_dim, out_dim=out_dim, num=num_intervals, k=k, noise_scale=noise_scale, \n",
    "                                   scale_base=scale_base, scale_sp=scale_sp, base_fun=base_fun, grid_eps=grid_eps,\n",
    "                                   grid_range=grid_range, sp_trainable=sp_trainable, sb_trainable=sb_trainable, device=device).to(device)\n",
    "\n",
    "    def forward(self, X):\n",
    "        y_pred1, preacts1, postacts1, postspline1 = self.kan_layer1(X)\n",
    "        y_pred2, preacts2, postacts2, postspline2 = self.kan_layer2(y_pred1)\n",
    "        return y_pred2\n",
    "\n",
    "# Initialize the CustomKANModel\n",
    "in_dim = X_train.shape[1]  # 2073 in this case\n",
    "inter_dim = 118  # Example intermediate dimension\n",
    "out_dim = num_classes\n",
    "num_intervals = 7\n",
    "k = 3\n",
    "noise_scale = 0.1\n",
    "scale_base = 1.0\n",
    "scale_sp = 1.0\n",
    "base_fun = nn.SiLU()\n",
    "grid_eps = 0.02\n",
    "grid_range = [-1, 1]\n",
    "sp_trainable = True\n",
    "sb_trainable = True\n",
    "\n",
    "kan_model = CustomKANModel(in_dim=in_dim, inter_dim=inter_dim, out_dim=out_dim, num_intervals=num_intervals, k=k, noise_scale=noise_scale, \n",
    "                            scale_base=scale_base, scale_sp=scale_sp, base_fun=base_fun, grid_eps=grid_eps, grid_range=grid_range, \n",
    "                            sp_trainable=sp_trainable, sb_trainable=sb_trainable, device=device).to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(kan_model.parameters(), lr=0.01)\n",
    "\n",
    "# Sparsification parameters\n",
    "lambda_l1 = 1e-4\n",
    "lambda_entropy = 1e-4\n",
    "\n",
    "# Helper function to compute sparsification penalties\n",
    "def compute_sparsification_penalty(model):\n",
    "    l1_penalty = 0.0\n",
    "    entropy_penalty = 0.0\n",
    "    for param in model.parameters():\n",
    "        l1_penalty += torch.sum(torch.abs(param))\n",
    "        param_normalized = torch.abs(param) / torch.sum(torch.abs(param))\n",
    "        entropy_penalty -= torch.sum(param_normalized * torch.log(param_normalized + 1e-10))\n",
    "    return lambda_l1 * l1_penalty + lambda_entropy * entropy_penalty\n",
    "\n",
    "# Training loop with sparsification\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    kan_model.train()\n",
    "    train_losses = []\n",
    "    train_correct = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    # Training phase\n",
    "    for X_batch, y_batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False):\n",
    "        # Forward pass\n",
    "        y_pred = kan_model(X_batch)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        \n",
    "        # Compute sparsification penalty\n",
    "        sparsification_penalty = compute_sparsification_penalty(kan_model)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = loss + sparsification_penalty\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(total_loss.item())\n",
    "        _, predicted = torch.max(y_pred.data, 1)\n",
    "        total_train += y_batch.size(0)\n",
    "        train_correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    train_accuracy = 100 * train_correct / total_train\n",
    "    \n",
    "    # Testing phase\n",
    "    kan_model.eval()\n",
    "    test_losses = []\n",
    "    test_correct = 0\n",
    "    total_test = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            y_pred = kan_model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            test_losses.append(loss.item())\n",
    "            _, predicted = torch.max(y_pred.data, 1)\n",
    "            total_test += y_batch.size(0)\n",
    "            test_correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    test_accuracy = 100 * test_correct / total_test\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {np.mean(train_losses):.4f}, Train Accuracy: {train_accuracy:.2f}%, Test Loss: {np.mean(test_losses):.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "# Pruning function\n",
    "def prune_kan_layer(kan_layer, threshold=1e-2):\n",
    "    with torch.no_grad():\n",
    "        for name, param in kan_layer.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                param *= (torch.abs(param) > threshold).float()\n",
    "\n",
    "# Prune the KAN layers after training\n",
    "prune_kan_layer(kan_model.kan_layer1)\n",
    "prune_kan_layer(kan_model.kan_layer2)\n",
    "\n",
    "# Retrain the pruned KAN model\n",
    "for epoch in range(num_epochs):\n",
    "    kan_model.train()\n",
    "    train_losses = []\n",
    "    train_correct = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    # Training phase\n",
    "    for X_batch, y_batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False):\n",
    "        # Forward pass\n",
    "        y_pred = kan_model(X_batch)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        \n",
    "        # Compute sparsification penalty\n",
    "        sparsification_penalty = compute_sparsification_penalty(kan_model)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = loss + sparsification_penalty\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(total_loss.item())\n",
    "        _, predicted = torch.max(y_pred.data, 1)\n",
    "        total_train += y_batch.size(0)\n",
    "        train_correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    train_accuracy = 100 * train_correct / total_train\n",
    "    \n",
    "    # Testing phase\n",
    "    kan_model.eval()\n",
    "    test_losses = []\n",
    "    test_correct = 0\n",
    "    total_test = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            y_pred = kan_model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            test_losses.append(loss.item())\n",
    "            _, predicted = torch.max(y_pred.data, 1)\n",
    "            total_test += y_batch.size(0)\n",
    "            test_correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    test_accuracy = 100 * test_correct / total_test\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {np.mean(train_losses):.4f}, Train Accuracy: {train_accuracy:.2f}%, Test Loss: {np.mean(test_losses):.4f}, Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4736783f-a7dd-4562-a912-cafeeb50ee4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "trainer = KANModelTrainer(mouse_number=715093703, output_dir=\"output\", timesteps_per_frame=1, batch_size=16, num_epochs=100)\n",
    "trainer.set_hyperparameters(inter_dim=118, num_layers=3, lr=0.01)\n",
    "trainer.prepare_data()\n",
    "trainer.create_kan_model()\n",
    "\n",
    "# Number of repetitions\n",
    "num_repetitions = 3\n",
    "\n",
    "# Repeat training and pruning\n",
    "for i in range(num_repetitions):\n",
    "    print(f\"Repetition {i+1}/{num_repetitions}\")\n",
    "    trainer.train()\n",
    "    trainer.prune_and_retrain()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
