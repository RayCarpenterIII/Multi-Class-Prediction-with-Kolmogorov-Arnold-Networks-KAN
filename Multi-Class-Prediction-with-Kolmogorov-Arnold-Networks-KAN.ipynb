{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00881cd0-8959-4818-9470-89d4445e561b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### KAN For Multivariate Classification in Predicting Visual Stimuli Given Firing Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0dbb940-a050-4203-817a-ef408e9b3667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated version 3!\n",
      "Initializing workflow...\n",
      "Loading existing datasets...\n",
      "Loaded spike trains dataset: <class 'pandas.core.frame.DataFrame'>\n",
      "Total time elapsed: 0.01 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Train Loss: 195.3483, Train Accuracy: 20.04%, Test Loss: 4.1914, Test Accuracy: 51.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Train Loss: 163.6227, Train Accuracy: 77.78%, Test Loss: 2.0736, Test Accuracy: 58.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Train Loss: 152.0205, Train Accuracy: 86.72%, Test Loss: 1.6183, Test Accuracy: 67.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Train Loss: 145.4312, Train Accuracy: 88.05%, Test Loss: 1.5204, Test Accuracy: 70.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Train Loss: 141.8137, Train Accuracy: 88.22%, Test Loss: 1.5012, Test Accuracy: 71.86%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from kan import KANLayer  # Assuming kan is the module containing KANLayer\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from data_processors.pull_and_process_data import master_function\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Hyper-parameters\n",
    "batch_size = 16\n",
    "num_epochs = 5\n",
    "num_intervals = 8\n",
    "k = 6\n",
    "noise_scale = 0.1\n",
    "scale_base = 1.0\n",
    "scale_sp = 1.0\n",
    "base_fun = nn.SiLU()\n",
    "grid_eps = 0.0\n",
    "grid_range = [0, 1]\n",
    "sp_trainable = True\n",
    "sb_trainable = True\n",
    "# Sparsification parameters\n",
    "lambda_l1 = 1e-4\n",
    "lambda_entropy = 1e-4\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Pull and process data\n",
    "mouse_number = 715093703\n",
    "spike_df = master_function(session_number=mouse_number, output_dir=\"output\", timesteps_per_frame=1)\n",
    "spike_df = spike_df[spike_df['frame'] >= 0]\n",
    "\n",
    "# Prepare data\n",
    "X = spike_df.drop('frame', axis=1).values\n",
    "y = spike_df['frame'].values.astype(int)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "X = X_normalized\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.from_numpy(X_train).float().to(device)\n",
    "y_train = torch.from_numpy(y_train).long().to(device)\n",
    "X_test = torch.from_numpy(X_test).float().to(device)\n",
    "y_test = torch.from_numpy(y_test).long().to(device)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the KANLayer\n",
    "in_dim = X_train.shape[1]  # 2073 in this case\n",
    "out_dim = num_classes\n",
    "\n",
    "kan_layer = KANLayer(in_dim=in_dim, out_dim=out_dim, num=num_intervals, k=k, noise_scale=noise_scale, \n",
    "                     scale_base=scale_base, scale_sp=scale_sp, base_fun=base_fun, grid_eps=grid_eps,\n",
    "                     grid_range=grid_range, sp_trainable=sp_trainable, sb_trainable=sb_trainable, device=device).to(device)\n",
    "\n",
    "# Example forward pass\n",
    "def forward_pass(kan_layer, X):\n",
    "    y_pred, preacts, postacts, postspline = kan_layer(X)\n",
    "    return y_pred\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(kan_layer.parameters(), lr=0.01)\n",
    "\n",
    "# Helper function to compute sparsification penalties\n",
    "def compute_sparsification_penalty(kan_layer):\n",
    "    l1_penalty = 0.0\n",
    "    entropy_penalty = 0.0\n",
    "    for param in kan_layer.parameters():\n",
    "        l1_penalty += torch.sum(torch.abs(param))\n",
    "        param_normalized = torch.abs(param) / torch.sum(torch.abs(param))\n",
    "        entropy_penalty -= torch.sum(param_normalized * torch.log(param_normalized + 1e-10))\n",
    "    return lambda_l1 * l1_penalty + lambda_entropy * entropy_penalty\n",
    "\n",
    "# Training loop with sparsification\n",
    "for epoch in range(num_epochs):\n",
    "    kan_layer.train()\n",
    "    train_losses = []\n",
    "    train_correct = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    # Training phase\n",
    "    for X_batch, y_batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False):\n",
    "        # Forward pass\n",
    "        y_pred = forward_pass(kan_layer, X_batch)\n",
    "        # Compute loss\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        \n",
    "        # Compute sparsification penalty\n",
    "        sparsification_penalty = compute_sparsification_penalty(kan_layer)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = loss + sparsification_penalty\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(total_loss.item())\n",
    "        _, predicted = torch.max(y_pred.data, 1)\n",
    "        total_train += y_batch.size(0)\n",
    "        train_correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    train_accuracy = 100 * train_correct / total_train\n",
    "    \n",
    "    # Testing phase\n",
    "    kan_layer.eval()\n",
    "    test_losses = []\n",
    "    test_correct = 0\n",
    "    total_test = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            y_pred = forward_pass(kan_layer, X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            test_losses.append(loss.item())\n",
    "            _, predicted = torch.max(y_pred.data, 1)\n",
    "            total_test += y_batch.size(0)\n",
    "            test_correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    test_accuracy = 100 * test_correct / total_test\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {np.mean(train_losses):.4f}, Train Accuracy: {train_accuracy:.2f}%, Test Loss: {np.mean(test_losses):.4f}, Test Accuracy: {test_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c16fcb89-0c1e-46f0-8f43-57228364b951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated version 3!\n",
      "Initializing workflow...\n",
      "Loading existing datasets...\n",
      "Loaded spike trains dataset: <class 'pandas.core.frame.DataFrame'>\n",
      "Total time elapsed: 0.01 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Train Loss: 10.2611, Train Accuracy: 38.03%, Test Loss: 2.1334, Test Accuracy: 58.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Train Loss: 7.8033, Train Accuracy: 75.70%, Test Loss: 1.8754, Test Accuracy: 62.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Train Loss: 7.0614, Train Accuracy: 80.04%, Test Loss: 1.8434, Test Accuracy: 61.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Train Loss: 6.7371, Train Accuracy: 81.42%, Test Loss: 1.8541, Test Accuracy: 60.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Train Loss: 6.5690, Train Accuracy: 81.48%, Test Loss: 1.8588, Test Accuracy: 60.25%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from kan import KANLayer  # Assuming kan is the module containing KANLayer\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from data_processors.pull_and_process_data import master_function\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Hyper-parameters\n",
    "batch_size = 16\n",
    "num_epochs = 5\n",
    "num_intervals = 3\n",
    "k = 3\n",
    "noise_scale = 0.1\n",
    "scale_base = 1.0\n",
    "scale_sp = 1.0\n",
    "base_fun = nn.SiLU()\n",
    "grid_eps = 0.0\n",
    "grid_range = [0, 1]\n",
    "sp_trainable = True\n",
    "sb_trainable = True\n",
    "# Sparsification parameters\n",
    "lambda_l1 = 1e-4\n",
    "lambda_entropy = 1e-4\n",
    "n_components = 120  # Adjust this based on your requirements\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Pull and process data\n",
    "mouse_number = 715093703\n",
    "spike_df = master_function(session_number=mouse_number, output_dir=\"output\", timesteps_per_frame=1)\n",
    "spike_df = spike_df[spike_df['frame'] >= 0]\n",
    "\n",
    "# Prepare data\n",
    "X = spike_df.drop('frame', axis=1).values\n",
    "y = spike_df['frame'].values.astype(int)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "X = X_normalized\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Perform PCA to reduce dimensions\n",
    "pca = PCA(n_components=n_components)\n",
    "X_train_reduced = pca.fit_transform(X_train)\n",
    "X_test_reduced = pca.transform(X_test)\n",
    "\n",
    "# Convert reduced data back to PyTorch tensors\n",
    "X_train_reduced = torch.from_numpy(X_train_reduced).float().to(device)\n",
    "X_test_reduced = torch.from_numpy(X_test_reduced).float().to(device)\n",
    "y_train = torch.from_numpy(y_train).long().to(device)\n",
    "y_test = torch.from_numpy(y_test).long().to(device)\n",
    "\n",
    "# Create DataLoader with reduced data\n",
    "train_dataset_reduced = TensorDataset(X_train_reduced, y_train)\n",
    "test_dataset_reduced = TensorDataset(X_test_reduced, y_test)\n",
    "train_loader_reduced = DataLoader(train_dataset_reduced, batch_size=batch_size, shuffle=True)\n",
    "test_loader_reduced = DataLoader(test_dataset_reduced, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the KANLayer with reduced input dimension\n",
    "in_dim_reduced = X_train_reduced.shape[1]  # This should be equal to n_components\n",
    "out_dim = num_classes\n",
    "\n",
    "kan_layer_reduced = KANLayer(in_dim=in_dim_reduced, out_dim=out_dim, num=num_intervals, k=k, noise_scale=noise_scale, \n",
    "                             scale_base=scale_base, scale_sp=scale_sp, base_fun=base_fun, grid_eps=grid_eps,\n",
    "                             grid_range=grid_range, sp_trainable=sp_trainable, sb_trainable=sb_trainable, device=device).to(device)\n",
    "\n",
    "# Example forward pass\n",
    "def forward_pass(kan_layer, X):\n",
    "    y_pred, preacts, postacts, postspline = kan_layer(X)\n",
    "    return y_pred\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(kan_layer_reduced.parameters(), lr=0.01)\n",
    "\n",
    "# Helper function to compute sparsification penalties\n",
    "def compute_sparsification_penalty(kan_layer):\n",
    "    l1_penalty = 0.0\n",
    "    entropy_penalty = 0.0\n",
    "    for param in kan_layer.parameters():\n",
    "        l1_penalty += torch.sum(torch.abs(param))\n",
    "        param_normalized = torch.abs(param) / torch.sum(torch.abs(param))\n",
    "        entropy_penalty -= torch.sum(param_normalized * torch.log(param_normalized + 1e-10))\n",
    "    return lambda_l1 * l1_penalty + lambda_entropy * entropy_penalty\n",
    "\n",
    "# Training loop with reduced data\n",
    "for epoch in range(num_epochs):\n",
    "    kan_layer_reduced.train()\n",
    "    train_losses = []\n",
    "    train_correct = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    # Training phase\n",
    "    for X_batch, y_batch in tqdm(train_loader_reduced, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False):\n",
    "        # Forward pass\n",
    "        y_pred = forward_pass(kan_layer_reduced, X_batch)\n",
    "        # Compute loss\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        \n",
    "        # Compute sparsification penalty\n",
    "        sparsification_penalty = compute_sparsification_penalty(kan_layer_reduced)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = loss + sparsification_penalty\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(total_loss.item())\n",
    "        _, predicted = torch.max(y_pred.data, 1)\n",
    "        total_train += y_batch.size(0)\n",
    "        train_correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    train_accuracy = 100 * train_correct / total_train\n",
    "    \n",
    "    # Testing phase\n",
    "    kan_layer_reduced.eval()\n",
    "    test_losses = []\n",
    "    test_correct = 0\n",
    "    total_test = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader_reduced:\n",
    "            y_pred = forward_pass(kan_layer_reduced, X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            test_losses.append(loss.item())\n",
    "            _, predicted = torch.max(y_pred.data, 1)\n",
    "            total_test += y_batch.size(0)\n",
    "            test_correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    test_accuracy = 100 * test_correct / total_test\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {np.mean(train_losses):.4f}, Train Accuracy: {train_accuracy:.2f}%, Test Loss: {np.mean(test_losses):.4f}, Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5128698c-693e-4f48-ab79-e84f13b6f123",
   "metadata": {
    "tags": []
   },
   "source": [
    "### KAN Symbolic layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b975fcb9-2124-430f-8df6-b42957a15510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated version 3!\n",
      "Initializing workflow...\n",
      "Loading existing datasets...\n",
      "Loaded spike trains dataset: <class 'pandas.core.frame.DataFrame'>\n",
      "Total time elapsed: 0.01 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Train Loss: 10.2471, Train Accuracy: 37.84%, Test Loss: 2.2095, Test Accuracy: 55.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Train Loss: 7.7887, Train Accuracy: 75.68%, Test Loss: 1.9351, Test Accuracy: 60.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Train Loss: 7.0602, Train Accuracy: 80.44%, Test Loss: 1.8320, Test Accuracy: 62.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Train Loss: 6.7352, Train Accuracy: 80.91%, Test Loss: 1.8876, Test Accuracy: 60.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Train Loss: 6.5696, Train Accuracy: 81.55%, Test Loss: 1.8697, Test Accuracy: 61.19%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from kan import KANLayer  # Assuming kan is the module containing KANLayer\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from data_processors.pull_and_process_data import master_function\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Hyper-parameters\n",
    "batch_size = 16\n",
    "num_epochs = 5\n",
    "num_intervals = 3\n",
    "k = 3\n",
    "noise_scale = 0.1\n",
    "scale_base = 1.0\n",
    "scale_sp = 1.0\n",
    "base_fun = nn.SiLU()\n",
    "grid_eps = 0.0\n",
    "grid_range = [0, 1]\n",
    "sp_trainable = True\n",
    "sb_trainable = True\n",
    "lambda_l1 = 1e-4\n",
    "lambda_entropy = 1e-4\n",
    "n_components = 120  # Adjust this based on your requirements\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Pull and process data\n",
    "mouse_number = 715093703\n",
    "spike_df = master_function(session_number=mouse_number, output_dir=\"output\", timesteps_per_frame=1)\n",
    "spike_df = spike_df[spike_df['frame'] >= 0]\n",
    "\n",
    "# Prepare data\n",
    "X = spike_df.drop('frame', axis=1).values\n",
    "y = spike_df['frame'].values.astype(int)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "X = X_normalized\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Perform PCA to reduce dimensions\n",
    "pca = PCA(n_components=n_components)\n",
    "X_train_reduced = pca.fit_transform(X_train)\n",
    "X_test_reduced = pca.transform(X_test)\n",
    "\n",
    "# Convert reduced data back to PyTorch tensors\n",
    "X_train_reduced = torch.from_numpy(X_train_reduced).float().to(device)\n",
    "X_test_reduced = torch.from_numpy(X_test_reduced).float().to(device)\n",
    "y_train = torch.from_numpy(y_train).long().to(device)\n",
    "y_test = torch.from_numpy(y_test).long().to(device)\n",
    "\n",
    "# Create DataLoader with reduced data\n",
    "train_dataset_reduced = TensorDataset(X_train_reduced, y_train)\n",
    "test_dataset_reduced = TensorDataset(X_test_reduced, y_test)\n",
    "train_loader_reduced = DataLoader(train_dataset_reduced, batch_size=batch_size, shuffle=True)\n",
    "test_loader_reduced = DataLoader(test_dataset_reduced, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the KANLayer with reduced input dimension\n",
    "in_dim_reduced = X_train_reduced.shape[1]  # This should be equal to n_components\n",
    "out_dim = num_classes\n",
    "\n",
    "kan_layer_reduced = KANLayer(in_dim=in_dim_reduced, out_dim=out_dim, num=num_intervals, k=k, noise_scale=noise_scale, \n",
    "                             scale_base=scale_base, scale_sp=scale_sp, base_fun=base_fun, grid_eps=grid_eps,\n",
    "                             grid_range=grid_range, sp_trainable=sp_trainable, sb_trainable=sb_trainable, device=device).to(device)\n",
    "\n",
    "# Example forward pass\n",
    "def forward_pass(kan_layer, X):\n",
    "    y_pred, preacts, postacts, postspline = kan_layer(X)\n",
    "    return y_pred\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(kan_layer_reduced.parameters(), lr=0.01)\n",
    "\n",
    "# Helper function to compute sparsification penalties\n",
    "def compute_sparsification_penalty(kan_layer):\n",
    "    l1_penalty = 0.0\n",
    "    entropy_penalty = 0.0\n",
    "    for param in kan_layer.parameters():\n",
    "        l1_penalty += torch.sum(torch.abs(param))\n",
    "        param_normalized = torch.abs(param) / torch.sum(torch.abs(param))\n",
    "        entropy_penalty -= torch.sum(param_normalized * torch.log(param_normalized + 1e-10))\n",
    "    return lambda_l1 * l1_penalty + lambda_entropy * entropy_penalty\n",
    "\n",
    "# Training loop with reduced data\n",
    "for epoch in range(num_epochs):\n",
    "    kan_layer_reduced.train()\n",
    "    train_losses = []\n",
    "    train_correct = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    # Training phase\n",
    "    for X_batch, y_batch in tqdm(train_loader_reduced, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False):\n",
    "        # Forward pass\n",
    "        y_pred = forward_pass(kan_layer_reduced, X_batch)\n",
    "        # Compute loss\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        \n",
    "        # Compute sparsification penalty\n",
    "        sparsification_penalty = compute_sparsification_penalty(kan_layer_reduced)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = loss + sparsification_penalty\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(total_loss.item())\n",
    "        _, predicted = torch.max(y_pred.data, 1)\n",
    "        total_train += y_batch.size(0)\n",
    "        train_correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    train_accuracy = 100 * train_correct / total_train\n",
    "    \n",
    "    # Testing phase\n",
    "    kan_layer_reduced.eval()\n",
    "    test_losses = []\n",
    "    test_correct = 0\n",
    "    total_test = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader_reduced:\n",
    "            y_pred = forward_pass(kan_layer_reduced, X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            test_losses.append(loss.item())\n",
    "            _, predicted = torch.max(y_pred.data, 1)\n",
    "            total_test += y_batch.size(0)\n",
    "            test_correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    test_accuracy = 100 * test_correct / total_test\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {np.mean(train_losses):.4f}, Train Accuracy: {train_accuracy:.2f}%, Test Loss: {np.mean(test_losses):.4f}, Test Accuracy: {test_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c42d05fc-8d52-48f7-81d2-5e5a2f80774c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated version 3!\n",
      "Initializing workflow...\n",
      "Loading existing datasets...\n",
      "Loaded spike trains dataset: <class 'pandas.core.frame.DataFrame'>\n",
      "Total time elapsed: 0.02 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_893949/2001205193.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'Epoch {epoch+1}/{num_epochs}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_kan_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0;31m# Compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_893949/2001205193.py\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(symbolic_kan_layer, X)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;31m# Example forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_kan_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostacts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msymbolic_kan_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/kan/Symbolic_KANLayer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mpostacts_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                 \u001b[0mxij\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maffine\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maffine\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maffine\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maffine\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                 \u001b[0mpostacts_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mxij\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mpostacts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpostacts_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from kan import Symbolic_KANLayer  # Assuming kan is the module containing Symbolic_KANLayer\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from data_processors.pull_and_process_data import master_function\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Hyper-parameters\n",
    "batch_size = 16\n",
    "num_epochs = 5\n",
    "num_intervals = 4\n",
    "k = 3\n",
    "noise_scale = 0.1\n",
    "scale_base = 1.0\n",
    "scale_sp = 1.0\n",
    "base_fun = nn.SiLU()\n",
    "grid_eps = 0.0\n",
    "grid_range = [0, 1]\n",
    "sp_trainable = True\n",
    "sb_trainable = True\n",
    "# Sparsification parameters\n",
    "lambda_l1 = 1e-4\n",
    "lambda_entropy = 1e-4\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Pull and process data\n",
    "mouse_number = 715093703\n",
    "spike_df = master_function(session_number=mouse_number, output_dir=\"output\", timesteps_per_frame=1)\n",
    "spike_df = spike_df[spike_df['frame'] >= 0]\n",
    "\n",
    "# Prepare data\n",
    "X = spike_df.drop('frame', axis=1).values\n",
    "y = spike_df['frame'].values.astype(int)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "X = X_normalized\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.from_numpy(X_train).float().to(device)\n",
    "y_train = torch.from_numpy(y_train).long().to(device)\n",
    "X_test = torch.from_numpy(X_test).float().to(device)\n",
    "y_test = torch.from_numpy(y_test).long().to(device)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the Symbolic_KANLayer\n",
    "in_dim = X_train.shape[1]  # 2073 in this case\n",
    "out_dim = num_classes\n",
    "\n",
    "symbolic_kan_layer = Symbolic_KANLayer(in_dim=in_dim, out_dim=out_dim, device=device).to(device)\n",
    "\n",
    "# Define symbolic functions for the layer (example, you can modify as needed)\n",
    "for i in range(in_dim):\n",
    "    for j in range(out_dim):\n",
    "        symbolic_kan_layer.fix_symbolic(i, j, 'sin')  # Replace 'sin' with the desired symbolic function\n",
    "\n",
    "# Example forward pass\n",
    "def forward_pass(symbolic_kan_layer, X):\n",
    "    y_pred, postacts = symbolic_kan_layer(X)\n",
    "    return y_pred\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(symbolic_kan_layer.parameters(), lr=0.01)\n",
    "\n",
    "# Helper function to compute sparsification penalties\n",
    "def compute_sparsification_penalty(symbolic_kan_layer):\n",
    "    l1_penalty = 0.0\n",
    "    entropy_penalty = 0.0\n",
    "    for param in symbolic_kan_layer.parameters():\n",
    "        l1_penalty += torch.sum(torch.abs(param))\n",
    "        param_normalized = torch.abs(param) / torch.sum(torch.abs(param))\n",
    "        entropy_penalty -= torch.sum(param_normalized * torch.log(param_normalized + 1e-10))\n",
    "    return lambda_l1 * l1_penalty + lambda_entropy * entropy_penalty\n",
    "\n",
    "# Training loop with sparsification\n",
    "for epoch in range(num_epochs):\n",
    "    symbolic_kan_layer.train()\n",
    "    train_losses = []\n",
    "    train_correct = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    # Training phase\n",
    "    for X_batch, y_batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False):\n",
    "        # Forward pass\n",
    "        y_pred = forward_pass(symbolic_kan_layer, X_batch)\n",
    "        # Compute loss\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        \n",
    "        # Compute sparsification penalty\n",
    "        sparsification_penalty = compute_sparsification_penalty(symbolic_kan_layer)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = loss + sparsification_penalty\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(total_loss.item())\n",
    "        _, predicted = torch.max(y_pred.data, 1)\n",
    "        total_train += y_batch.size(0)\n",
    "        train_correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    train_accuracy = 100 * train_correct / total_train\n",
    "    \n",
    "    # Testing phase\n",
    "    symbolic_kan_layer.eval()\n",
    "    test_losses = []\n",
    "    test_correct = 0\n",
    "    total_test = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            y_pred = forward_pass(symbolic_kan_layer, X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            test_losses.append(loss.item())\n",
    "            _, predicted = torch.max(y_pred.data, 1)\n",
    "            total_test += y_batch.size(0)\n",
    "            test_correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    test_accuracy = 100 * test_correct / total_test\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {np.mean(train_losses):.4f}, Train Accuracy: {train_accuracy:.2f}%, Test Loss: {np.mean(test_losses):.4f}, Test Accuracy: {test_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e60be7d1-2345-4e55-ba3a-5ce88518dd64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated version 3!\n",
      "Initializing workflow...\n",
      "Loading existing datasets...\n",
      "Loaded spike trains dataset: <class 'pandas.core.frame.DataFrame'>\n",
      "Total time elapsed: 0.02 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_893949/696194172.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'Epoch {epoch+1}/{num_epochs}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;31m# Compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_893949/696194172.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mreduced_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostacts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbolic_kan_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduced_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostacts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/kan/Symbolic_KANLayer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mpostacts_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                 \u001b[0mxij\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maffine\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maffine\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maffine\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maffine\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                 \u001b[0mpostacts_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mxij\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mpostacts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpostacts_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from kan import Symbolic_KANLayer  # Assuming kan is the module containing Symbolic_KANLayer\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from data_processors.pull_and_process_data import master_function\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Hyper-parameters\n",
    "batch_size = 16\n",
    "num_epochs = 5\n",
    "num_intervals = 4\n",
    "k = 4\n",
    "noise_scale = 0.1\n",
    "scale_base = 1.0\n",
    "scale_sp = 1.0\n",
    "base_fun = nn.SiLU()\n",
    "grid_eps = 0.0\n",
    "grid_range = [0, 1]\n",
    "sp_trainable = True\n",
    "sb_trainable = True\n",
    "# Sparsification parameters\n",
    "lambda_l1 = 1e-4\n",
    "lambda_entropy = 1e-4\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Pull and process data\n",
    "mouse_number = 715093703\n",
    "spike_df = master_function(session_number=mouse_number, output_dir=\"output\", timesteps_per_frame=1)\n",
    "spike_df = spike_df[spike_df['frame'] >= 0]\n",
    "\n",
    "# Prepare data\n",
    "X = spike_df.drop('frame', axis=1).values\n",
    "y = spike_df['frame'].values.astype(int)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "X = X_normalized\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.from_numpy(X_train).float().to(device)\n",
    "y_train = torch.from_numpy(y_train).long().to(device)\n",
    "X_test = torch.from_numpy(X_test).float().to(device)\n",
    "y_test = torch.from_numpy(y_test).long().to(device)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define a model with a linear layer followed by a Symbolic_KANLayer\n",
    "class DimensionalityReductionModel(nn.Module):\n",
    "    def __init__(self, input_dim, reduced_dim, symbolic_kan_layer):\n",
    "        super(DimensionalityReductionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, reduced_dim)\n",
    "        self.symbolic_kan_layer = symbolic_kan_layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        reduced_x = self.linear(x)\n",
    "        y_pred, postacts = self.symbolic_kan_layer(reduced_x)\n",
    "        return y_pred, postacts\n",
    "\n",
    "# Initialize the Symbolic_KANLayer\n",
    "in_dim = X_train.shape[1]  # 2073 in this case\n",
    "reduced_dim = 512  # You can choose an appropriate reduced dimension\n",
    "out_dim = num_classes\n",
    "\n",
    "symbolic_kan_layer = Symbolic_KANLayer(in_dim=reduced_dim, out_dim=out_dim, device=device).to(device)\n",
    "\n",
    "# Define symbolic functions for the layer (example, you can modify as needed)\n",
    "for i in range(reduced_dim):\n",
    "    for j in range(out_dim):\n",
    "        symbolic_kan_layer.fix_symbolic(i, j, 'sin')  # Replace 'sin' with the desired symbolic function\n",
    "\n",
    "# Initialize the full model\n",
    "model = DimensionalityReductionModel(in_dim, reduced_dim, symbolic_kan_layer).to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Helper function to compute sparsification penalties\n",
    "def compute_sparsification_penalty(symbolic_kan_layer):\n",
    "    l1_penalty = 0.0\n",
    "    entropy_penalty = 0.0\n",
    "    for param in symbolic_kan_layer.parameters():\n",
    "        l1_penalty += torch.sum(torch.abs(param))\n",
    "        param_normalized = torch.abs(param) / torch.sum(torch.abs(param))\n",
    "        entropy_penalty -= torch.sum(param_normalized * torch.log(param_normalized + 1e-10))\n",
    "    return lambda_l1 * l1_penalty + lambda_entropy * entropy_penalty\n",
    "\n",
    "# Training loop with sparsification\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    train_correct = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    # Training phase\n",
    "    for X_batch, y_batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False):\n",
    "        # Forward pass\n",
    "        y_pred, _ = model(X_batch)\n",
    "        # Compute loss\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        \n",
    "        # Compute sparsification penalty\n",
    "        sparsification_penalty = compute_sparsification_penalty(symbolic_kan_layer)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = loss + sparsification_penalty\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(total_loss.item())\n",
    "        _, predicted = torch.max(y_pred.data, 1)\n",
    "        total_train += y_batch.size(0)\n",
    "        train_correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    train_accuracy = 100 * train_correct / total_train\n",
    "    \n",
    "    # Testing phase\n",
    "    model.eval()\n",
    "    test_losses = []\n",
    "    test_correct = 0\n",
    "    total_test = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            y_pred, _ = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            test_losses.append(loss.item())\n",
    "            _, predicted = torch.max(y_pred.data, 1)\n",
    "            total_test += y_batch.size(0)\n",
    "            test_correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    test_accuracy = 100 * test_correct / total_test\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {np.mean(train_losses):.4f}, Train Accuracy: {train_accuracy:.2f}%, Test Loss: {np.mean(test_losses):.4f}, Test Accuracy: {test_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aef8ec-f3f5-4d04-b514-30035e08a0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from kan import Symbolic_KANLayer  # Assuming kan is the module containing KANLayer\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from data_processors.pull_and_process_data import master_function\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Hyper-parameters\n",
    "batch_size = 16\n",
    "num_epochs = 5\n",
    "num_intervals = 4\n",
    "k = 4\n",
    "noise_scale = .05\n",
    "scale_base = 1\n",
    "scale_sp = 1.0\n",
    "base_fun = nn.SiLU()\n",
    "grid_eps = 1\n",
    "grid_range = [0, 1]\n",
    "sp_trainable = True\n",
    "sb_trainable = True\n",
    "reduction_dim = 120  # New dimension after reduction\n",
    "# Sparsification parameters\n",
    "lambda_l1 = 1e-4\n",
    "lambda_entropy = 1e-4\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Pull and process data\n",
    "mouse_number = 732592105\n",
    "\n",
    "spike_df = master_function(session_number=mouse_number, output_dir=\"output\", timesteps_per_frame=1)\n",
    "spike_df = spike_df[spike_df['frame'] >= 0]\n",
    "\n",
    "# Prepare data\n",
    "X = spike_df.drop('frame', axis=1).values\n",
    "y = spike_df['frame'].values.astype(int)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "X = X_normalized\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.from_numpy(X_train).float().to(device)\n",
    "y_train = torch.from_numpy(y_train).long().to(device)\n",
    "X_test = torch.from_numpy(X_test).float().to(device)\n",
    "y_test = torch.from_numpy(y_test).long().to(device)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the model\n",
    "class ConnectomicsModel(nn.Module):\n",
    "    def __init__(self, in_dim, reduction_dim, out_dim, num_intervals, k, noise_scale, scale_base, scale_sp, base_fun, grid_eps, grid_range, sp_trainable, sb_trainable, device):\n",
    "        super(ConnectomicsModel, self).__init__()\n",
    "        self.fc = nn.Linear(in_dim, reduction_dim)\n",
    "        self.symbolic_kan = Symbolic_KANLayer(in_dim=reduction_dim, out_dim=out_dim, device=device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        y_pred, postacts = self.symbolic_kan(x)\n",
    "        return y_pred\n",
    "\n",
    "# Initialize the model\n",
    "model = ConnectomicsModel(\n",
    "    in_dim=X_train.shape[1], \n",
    "    reduction_dim=reduction_dim, \n",
    "    out_dim=num_classes, \n",
    "    num_intervals=num_intervals, \n",
    "    k=k, \n",
    "    noise_scale=noise_scale, \n",
    "    scale_base=scale_base, \n",
    "    scale_sp=scale_sp, \n",
    "    base_fun=base_fun, \n",
    "    grid_eps=grid_eps, \n",
    "    grid_range=grid_range, \n",
    "    sp_trainable=sp_trainable, \n",
    "    sb_trainable=sb_trainable, \n",
    "    device=device\n",
    ").to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Helper function to compute sparsification penalties\n",
    "def compute_sparsification_penalty(kan_layer):\n",
    "    l1_penalty = 0.0\n",
    "    entropy_penalty = 0.0\n",
    "    for param in kan_layer.parameters():\n",
    "        l1_penalty += torch.sum(torch.abs(param))\n",
    "        param_normalized = torch.abs(param) / torch.sum(torch.abs(param))\n",
    "        entropy_penalty -= torch.sum(param_normalized * torch.log(param_normalized + 1e-10))\n",
    "    return lambda_l1 * l1_penalty + lambda_entropy * entropy_penalty\n",
    "\n",
    "# Training loop with sparsification\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    train_correct = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    # Training phase\n",
    "    for X_batch, y_batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False):\n",
    "        # Forward pass\n",
    "        y_pred = model(X_batch)\n",
    "        # Compute loss\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        \n",
    "        # Compute sparsification penalty\n",
    "        sparsification_penalty = compute_sparsification_penalty(model.symbolic_kan)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = loss + sparsification_penalty\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(total_loss.item())\n",
    "        _, predicted = torch.max(y_pred.data, 1)\n",
    "        total_train += y_batch.size(0)\n",
    "        train_correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    train_accuracy = 100 * train_correct / total_train\n",
    "    \n",
    "    # Testing phase\n",
    "    model.eval()\n",
    "    test_losses = []\n",
    "    test_correct = 0\n",
    "    total_test = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            test_losses.append(loss.item())\n",
    "            _, predicted = torch.max(y_pred.data, 1)\n",
    "            total_test += y_batch.size(0)\n",
    "            test_correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    test_accuracy = 100 * test_correct / total_test\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {np.mean(train_losses):.4f}, Train Accuracy: {train_accuracy:.2f}%, Test Loss: {np.mean(test_losses):.4f}, Test Accuracy: {test_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1622a8ff-ac21-45e3-b597-757181a2109b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Multi-Layered KAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e15a6e-2c47-4bd0-ba87-24fc2ffc6ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from kan import KANLayer  # Assuming kan is the module containing KANLayer\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from data_processors.pull_and_process_data import master_function\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Pull and process data\n",
    "mouse_number = 715093703\n",
    "spike_df = master_function(session_number=mouse_number, output_dir=\"output\", timesteps_per_frame=1)\n",
    "spike_df = spike_df[spike_df['frame'] >= 0]\n",
    "\n",
    "# Prepare data\n",
    "X = spike_df.drop('frame', axis=1).values\n",
    "y = spike_df['frame'].values.astype(int)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "X = X_normalized\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.from_numpy(X_train).float().to(device)\n",
    "y_train = torch.from_numpy(y_train).long().to(device)\n",
    "X_test = torch.from_numpy(X_test).float().to(device)\n",
    "y_test = torch.from_numpy(y_test).long().to(device)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 16\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define a custom KAN model with an intermediate layer\n",
    "class CustomKANModel(nn.Module):\n",
    "    def __init__(self, in_dim, inter_dim, out_dim, num_intervals, k, noise_scale, scale_base, scale_sp, base_fun, grid_eps, grid_range, sp_trainable, sb_trainable, device):\n",
    "        super(CustomKANModel, self).__init__()\n",
    "        self.kan_layer1 = KANLayer(in_dim=in_dim, out_dim=inter_dim, num=num_intervals, k=k, noise_scale=noise_scale, \n",
    "                                   scale_base=scale_base, scale_sp=scale_sp, base_fun=base_fun, grid_eps=grid_eps,\n",
    "                                   grid_range=grid_range, sp_trainable=sp_trainable, sb_trainable=sb_trainable, device=device).to(device)\n",
    "        self.kan_layer2 = KANLayer(in_dim=inter_dim, out_dim=out_dim, num=num_intervals, k=k, noise_scale=noise_scale, \n",
    "                                   scale_base=scale_base, scale_sp=scale_sp, base_fun=base_fun, grid_eps=grid_eps,\n",
    "                                   grid_range=grid_range, sp_trainable=sp_trainable, sb_trainable=sb_trainable, device=device).to(device)\n",
    "\n",
    "    def forward(self, X):\n",
    "        y_pred1, preacts1, postacts1, postspline1 = self.kan_layer1(X)\n",
    "        y_pred2, preacts2, postacts2, postspline2 = self.kan_layer2(y_pred1)\n",
    "        return y_pred2\n",
    "\n",
    "# Initialize the CustomKANModel\n",
    "in_dim = X_train.shape[1]  # 2073 in this case\n",
    "inter_dim = 1000  # Example intermediate dimension\n",
    "out_dim = num_classes\n",
    "num_intervals = 8\n",
    "k = 6\n",
    "noise_scale = 0.1\n",
    "scale_base = 1.0\n",
    "scale_sp = 1.0\n",
    "base_fun = nn.SiLU()\n",
    "grid_eps = 1\n",
    "grid_range = [0, 1]\n",
    "sp_trainable = True\n",
    "sb_trainable = True\n",
    "\n",
    "kan_model = CustomKANModel(in_dim=in_dim, inter_dim=inter_dim, out_dim=out_dim, num_intervals=num_intervals, k=k, noise_scale=noise_scale, \n",
    "                            scale_base=scale_base, scale_sp=scale_sp, base_fun=base_fun, grid_eps=grid_eps, grid_range=grid_range, \n",
    "                            sp_trainable=sp_trainable, sb_trainable=sb_trainable, device=device).to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(kan_model.parameters(), lr=0.01)\n",
    "\n",
    "# Sparsification parameters\n",
    "lambda_l1 = 1e-4\n",
    "lambda_entropy = 1e-4\n",
    "\n",
    "# Helper function to compute sparsification penalties\n",
    "def compute_sparsification_penalty(model):\n",
    "    l1_penalty = 0.0\n",
    "    entropy_penalty = 0.0\n",
    "    for param in model.parameters():\n",
    "        l1_penalty += torch.sum(torch.abs(param))\n",
    "        param_normalized = torch.abs(param) / torch.sum(torch.abs(param))\n",
    "        entropy_penalty -= torch.sum(param_normalized * torch.log(param_normalized + 1e-10))\n",
    "    return lambda_l1 * l1_penalty + lambda_entropy * entropy_penalty\n",
    "\n",
    "# Training loop with sparsification\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    kan_model.train()\n",
    "    train_losses = []\n",
    "    train_correct = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    # Training phase\n",
    "    for X_batch, y_batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False):\n",
    "        # Forward pass\n",
    "        y_pred = kan_model(X_batch)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        \n",
    "        # Compute sparsification penalty\n",
    "        sparsification_penalty = compute_sparsification_penalty(kan_model)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = loss + sparsification_penalty\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(total_loss.item())\n",
    "        _, predicted = torch.max(y_pred.data, 1)\n",
    "        total_train += y_batch.size(0)\n",
    "        train_correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    train_accuracy = 100 * train_correct / total_train\n",
    "    \n",
    "    # Testing phase\n",
    "    kan_model.eval()\n",
    "    test_losses = []\n",
    "    test_correct = 0\n",
    "    total_test = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            y_pred = kan_model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            test_losses.append(loss.item())\n",
    "            _, predicted = torch.max(y_pred.data, 1)\n",
    "            total_test += y_batch.size(0)\n",
    "            test_correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    test_accuracy = 100 * test_correct / total_test\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {np.mean(train_losses):.4f}, Train Accuracy: {train_accuracy:.2f}%, Test Loss: {np.mean(test_losses):.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "# Pruning function\n",
    "def prune_kan_layer(kan_layer, threshold=1e-2):\n",
    "    with torch.no_grad():\n",
    "        for name, param in kan_layer.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                param *= (torch.abs(param) > threshold).float()\n",
    "\n",
    "# Prune the KAN layers after training\n",
    "prune_kan_layer(kan_model.kan_layer1)\n",
    "prune_kan_layer(kan_model.kan_layer2)\n",
    "\n",
    "# Retrain the pruned KAN model\n",
    "for epoch in range(num_epochs):\n",
    "    kan_model.train()\n",
    "    train_losses = []\n",
    "    train_correct = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    # Training phase\n",
    "    for X_batch, y_batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False):\n",
    "        # Forward pass\n",
    "        y_pred = kan_model(X_batch)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        \n",
    "        # Compute sparsification penalty\n",
    "        sparsification_penalty = compute_sparsification_penalty(kan_model)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = loss + sparsification_penalty\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(total_loss.item())\n",
    "        _, predicted = torch.max(y_pred.data, 1)\n",
    "        total_train += y_batch.size(0)\n",
    "        train_correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    train_accuracy = 100 * train_correct / total_train\n",
    "    \n",
    "    # Testing phase\n",
    "    kan_model.eval()\n",
    "    test_losses = []\n",
    "    test_correct = 0\n",
    "    total_test = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            y_pred = kan_model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            test_losses.append(loss.item())\n",
    "            _, predicted = torch.max(y_pred.data, 1)\n",
    "            total_test += y_batch.size(0)\n",
    "            test_correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    test_accuracy = 100 * test_correct / total_test\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {np.mean(train_losses):.4f}, Train Accuracy: {train_accuracy:.2f}%, Test Loss: {np.mean(test_losses):.4f}, Test Accuracy: {test_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c742c10-7d0e-4cf9-a0d3-344c46571868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated version 3!\n",
      "Initializing workflow...\n",
      "Loading existing datasets...\n",
      "Loaded spike trains dataset: <class 'pandas.core.frame.DataFrame'>\n",
      "Total time elapsed: 0.01 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/6], Train Loss: 739.1743, Train Accuracy: 0.68%, Test Loss: 4.8930, Test Accuracy: 1.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/6], Train Loss: 573.5616, Train Accuracy: 1.50%, Test Loss: 4.7925, Test Accuracy: 3.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/6], Train Loss: 560.4291, Train Accuracy: 11.29%, Test Loss: 3.5963, Test Accuracy: 19.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/6], Train Loss: 558.4958, Train Accuracy: 38.33%, Test Loss: 2.2982, Test Accuracy: 47.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/6], Train Loss: 557.6904, Train Accuracy: 59.11%, Test Loss: 1.8608, Test Accuracy: 57.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/6], Train Loss: 557.2176, Train Accuracy: 68.94%, Test Loss: 1.8196, Test Accuracy: 56.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/6], Train Loss: 557.0008, Train Accuracy: 74.64%, Test Loss: 1.6900, Test Accuracy: 57.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/6], Train Loss: 556.9613, Train Accuracy: 76.97%, Test Loss: 1.6114, Test Accuracy: 60.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/6], Train Loss: 557.0994, Train Accuracy: 75.47%, Test Loss: 2.3321, Test Accuracy: 49.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_77036/3420617986.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_kan_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprune_and_retrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_77036/3420617986.py\u001b[0m in \u001b[0;36mprune_and_retrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'Epoch {epoch+1}/{self.num_epochs}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                 \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkan_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0msparsification_penalty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_sparsification_penalty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkan_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_77036/3420617986.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m                     \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m                     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/kan/KANLayer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mpreacts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mbase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shape (batch, size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoef2curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_sharing\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_sharing\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shape (size, batch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shape (batch, size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mpostspline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from kan import KANLayer  # Assuming kan is the module containing KANLayer\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from data_processors.pull_and_process_data import master_function\n",
    "from tqdm import tqdm\n",
    "\n",
    "class KANModelTrainer:\n",
    "    def __init__(self, mouse_number, output_dir, timesteps_per_frame, batch_size=16, num_epochs=100, device=None):\n",
    "        self.mouse_number = mouse_number\n",
    "        self.output_dir = output_dir\n",
    "        self.timesteps_per_frame = timesteps_per_frame\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.lambda_l1 = 1e-4\n",
    "        self.lambda_entropy = 1e-4\n",
    "\n",
    "        # Set default hyperparameters\n",
    "        self.in_dim = None\n",
    "        self.inter_dim = 250\n",
    "        self.out_dim = None\n",
    "        self.num_intervals = 7\n",
    "        self.k = 6\n",
    "        self.noise_scale = 0.1\n",
    "        self.scale_base = 1.0\n",
    "        self.scale_sp = 1.0\n",
    "        self.base_fun = nn.SiLU()\n",
    "        self.grid_eps = 0.02\n",
    "        self.grid_range = [0, 1]\n",
    "        self.sp_trainable = True\n",
    "        self.sb_trainable = True\n",
    "        self.num_layers = 3\n",
    "        self.lr = 0.01\n",
    "\n",
    "    def set_hyperparameters(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        spike_df = master_function(session_number=self.mouse_number, output_dir=self.output_dir, timesteps_per_frame=self.timesteps_per_frame)\n",
    "        spike_df = spike_df[spike_df['frame'] >= 0]\n",
    "\n",
    "        X = spike_df.drop('frame', axis=1).values\n",
    "        y = spike_df['frame'].values.astype(int)\n",
    "\n",
    "        scaler = MinMaxScaler()\n",
    "        X_normalized = scaler.fit_transform(X)\n",
    "        X = X_normalized\n",
    "        self.out_dim = len(np.unique(y))\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "        \n",
    "        self.in_dim = X_train.shape[1]\n",
    "\n",
    "        X_train = torch.from_numpy(X_train).float().to(self.device)\n",
    "        y_train = torch.from_numpy(y_train).long().to(self.device)\n",
    "        X_test = torch.from_numpy(X_test).float().to(self.device)\n",
    "        y_test = torch.from_numpy(y_test).long().to(self.device)\n",
    "\n",
    "        train_dataset = TensorDataset(X_train, y_train)\n",
    "        test_dataset = TensorDataset(X_test, y_test)\n",
    "        self.train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        self.test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "    def create_kan_model(self):\n",
    "        class CustomKANModel(nn.Module):\n",
    "            def __init__(self, in_dim, inter_dim, out_dim, num_intervals, k, noise_scale, scale_base, scale_sp, base_fun, grid_eps, grid_range, sp_trainable, sb_trainable, num_layers, device):\n",
    "                super(CustomKANModel, self).__init__()\n",
    "                self.layers = nn.ModuleList()\n",
    "                self.layers.append(KANLayer(in_dim=in_dim, out_dim=inter_dim, num=num_intervals, k=k, noise_scale=noise_scale, \n",
    "                                            scale_base=scale_base, scale_sp=scale_sp, base_fun=base_fun, grid_eps=grid_eps,\n",
    "                                            grid_range=grid_range, sp_trainable=sp_trainable, sb_trainable=sb_trainable, device=device).to(device))\n",
    "                for _ in range(num_layers - 2):\n",
    "                    self.layers.append(KANLayer(in_dim=inter_dim, out_dim=inter_dim, num=num_intervals, k=k, noise_scale=noise_scale, \n",
    "                                                scale_base=scale_base, scale_sp=scale_sp, base_fun=base_fun, grid_eps=grid_eps,\n",
    "                                                grid_range=grid_range, sp_trainable=sp_trainable, sb_trainable=sb_trainable, device=device).to(device))\n",
    "                self.layers.append(KANLayer(in_dim=inter_dim, out_dim=out_dim, num=num_intervals, k=k, noise_scale=noise_scale, \n",
    "                                            scale_base=scale_base, scale_sp=scale_sp, base_fun=base_fun, grid_eps=grid_eps,\n",
    "                                            grid_range=grid_range, sp_trainable=sp_trainable, sb_trainable=sb_trainable, device=device).to(device))\n",
    "            \n",
    "            def forward(self, X):\n",
    "                for layer in self.layers:\n",
    "                    y_pred, _, _, _ = layer(X)\n",
    "                    X = y_pred\n",
    "                return y_pred\n",
    "\n",
    "        self.kan_model = CustomKANModel(self.in_dim, self.inter_dim, self.out_dim, self.num_intervals, self.k, self.noise_scale, \n",
    "                                        self.scale_base, self.scale_sp, self.base_fun, self.grid_eps, self.grid_range, \n",
    "                                        self.sp_trainable, self.sb_trainable, self.num_layers, self.device).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.kan_model.parameters(), lr=self.lr)\n",
    "\n",
    "    def compute_sparsification_penalty(self, model):\n",
    "        l1_penalty = 0.0\n",
    "        entropy_penalty = 0.0\n",
    "        for param in model.parameters():\n",
    "            l1_penalty += torch.sum(torch.abs(param))\n",
    "            param_normalized = torch.abs(param) / torch.sum(torch.abs(param))\n",
    "            entropy_penalty -= torch.sum(param_normalized * torch.log(param_normalized + 1e-10))\n",
    "        return self.lambda_l1 * l1_penalty + self.lambda_entropy * entropy_penalty\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.kan_model.train()\n",
    "            train_losses = []\n",
    "            train_correct = 0\n",
    "            total_train = 0\n",
    "            \n",
    "            for X_batch, y_batch in tqdm(self.train_loader, desc=f'Epoch {epoch+1}/{self.num_epochs}', leave=False):\n",
    "                y_pred = self.kan_model(X_batch)\n",
    "                loss = self.criterion(y_pred, y_batch)\n",
    "                sparsification_penalty = self.compute_sparsification_penalty(self.kan_model)\n",
    "                total_loss = loss + sparsification_penalty\n",
    "                self.optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                train_losses.append(total_loss.item())\n",
    "                _, predicted = torch.max(y_pred.data, 1)\n",
    "                total_train += y_batch.size(0)\n",
    "                train_correct += (predicted == y_batch).sum().item()\n",
    "            \n",
    "            train_accuracy = 100 * train_correct / total_train\n",
    "            \n",
    "            self.kan_model.eval()\n",
    "            test_losses = []\n",
    "            test_correct = 0\n",
    "            total_test = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for X_batch, y_batch in self.test_loader:\n",
    "                    y_pred = self.kan_model(X_batch)\n",
    "                    loss = self.criterion(y_pred, y_batch)\n",
    "                    test_losses.append(loss.item())\n",
    "                    _, predicted = torch.max(y_pred.data, 1)\n",
    "                    total_test += y_batch.size(0)\n",
    "                    test_correct += (predicted == y_batch).sum().item()\n",
    "            \n",
    "            test_accuracy = 100 * test_correct / total_test\n",
    "            \n",
    "            print(f'Epoch [{epoch+1}/{self.num_epochs}], Train Loss: {np.mean(train_losses):.4f}, Train Accuracy: {train_accuracy:.2f}%, Test Loss: {np.mean(test_losses):.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "    def prune_kan_layer(self, kan_layer, threshold=1e-2):\n",
    "        with torch.no_grad():\n",
    "            for name, param in kan_layer.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    param *= (torch.abs(param) > threshold).float()\n",
    "\n",
    "    def prune_and_retrain(self):\n",
    "        for layer in self.kan_model.layers:\n",
    "            self.prune_kan_layer(layer)\n",
    "        \n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.kan_model.train()\n",
    "            train_losses = []\n",
    "            train_correct = 0\n",
    "            total_train = 0\n",
    "            \n",
    "            for X_batch, y_batch in tqdm(self.train_loader, desc=f'Epoch {epoch+1}/{self.num_epochs}', leave=False):\n",
    "                y_pred = self.kan_model(X_batch)\n",
    "                loss = self.criterion(y_pred, y_batch)\n",
    "                sparsification_penalty = self.compute_sparsification_penalty(self.kan_model)\n",
    "                total_loss = loss + sparsification_penalty\n",
    "                self.optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                train_losses.append(total_loss.item())\n",
    "                _, predicted = torch.max(y_pred.data, 1)\n",
    "                total_train += y_batch.size(0)\n",
    "                train_correct += (predicted == y_batch).sum().item()\n",
    "            \n",
    "            train_accuracy = 100 * train_correct / total_train\n",
    "            \n",
    "            self.kan_model.eval()\n",
    "            test_losses = []\n",
    "            test_correct = 0\n",
    "            total_test = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for X_batch, y_batch in self.test_loader:\n",
    "                    y_pred = self.kan_model(X_batch)\n",
    "                    loss = self.criterion(y_pred, y_batch)\n",
    "                    test_losses.append(loss.item())\n",
    "                    _, predicted = torch.max(y_pred.data, 1)\n",
    "                    total_test += y_batch.size(0)\n",
    "                    test_correct += (predicted == y_batch).sum().item()\n",
    "            \n",
    "            test_accuracy = 100 * test_correct / total_test\n",
    "            \n",
    "            print(f'Epoch [{epoch+1}/{self.num_epochs}], Train Loss: {np.mean(train_losses):.4f}, Train Accuracy: {train_accuracy:.2f}%, Test Loss: {np.mean(test_losses):.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "# Example usage\n",
    "trainer = KANModelTrainer(mouse_number=715093703, output_dir=\"output\", timesteps_per_frame=1, batch_size=16, num_epochs=6)\n",
    "trainer.set_hyperparameters(inter_dim=500, num_layers=1, lr=0.01)\n",
    "trainer.prepare_data()\n",
    "trainer.create_kan_model()\n",
    "trainer.train()\n",
    "trainer.prune_and_retrain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cec518a-b22d-4ba4-b475-6dbae6b83ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from kan import KANLayer  # Assuming kan is the module containing KANLayer\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from data_processors.pull_and_process_data import master_function\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Pull and process data\n",
    "mouse_number = 715093703\n",
    "spike_df = master_function(session_number=mouse_number, output_dir=\"output\", timesteps_per_frame=1)\n",
    "spike_df = spike_df[spike_df['frame'] >= 0]\n",
    "\n",
    "# Prepare data\n",
    "X = spike_df.drop('frame', axis=1).values\n",
    "y = spike_df['frame'].values.astype(int)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "X = X_normalized\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.from_numpy(X_train).float().to(device)\n",
    "y_train = torch.from_numpy(y_train).long().to(device)\n",
    "X_test = torch.from_numpy(X_test).float().to(device)\n",
    "y_test = torch.from_numpy(y_test).long().to(device)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 16\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define a custom KAN model with an intermediate layer\n",
    "class CustomKANModel(nn.Module):\n",
    "    def __init__(self, in_dim, inter_dim, out_dim, num_intervals, k, noise_scale, scale_base, scale_sp, base_fun, grid_eps, grid_range, sp_trainable, sb_trainable, device):\n",
    "        super(CustomKANModel, self).__init__()\n",
    "        self.kan_layer1 = KANLayer(in_dim=in_dim, out_dim=inter_dim, num=num_intervals, k=k, noise_scale=noise_scale, \n",
    "                                   scale_base=scale_base, scale_sp=scale_sp, base_fun=base_fun, grid_eps=grid_eps,\n",
    "                                   grid_range=grid_range, sp_trainable=sp_trainable, sb_trainable=sb_trainable, device=device).to(device)\n",
    "        self.kan_layer2 = KANLayer(in_dim=inter_dim, out_dim=out_dim, num=num_intervals, k=k, noise_scale=noise_scale, \n",
    "                                   scale_base=scale_base, scale_sp=scale_sp, base_fun=base_fun, grid_eps=grid_eps,\n",
    "                                   grid_range=grid_range, sp_trainable=sp_trainable, sb_trainable=sb_trainable, device=device).to(device)\n",
    "\n",
    "    def forward(self, X):\n",
    "        y_pred1, preacts1, postacts1, postspline1 = self.kan_layer1(X)\n",
    "        y_pred2, preacts2, postacts2, postspline2 = self.kan_layer2(y_pred1)\n",
    "        return y_pred2\n",
    "\n",
    "# Initialize the CustomKANModel\n",
    "in_dim = X_train.shape[1]  # 2073 in this case\n",
    "inter_dim = 118  # Example intermediate dimension\n",
    "out_dim = num_classes\n",
    "num_intervals = 7\n",
    "k = 3\n",
    "noise_scale = 0.1\n",
    "scale_base = 1.0\n",
    "scale_sp = 1.0\n",
    "base_fun = nn.SiLU()\n",
    "grid_eps = 0.02\n",
    "grid_range = [-1, 1]\n",
    "sp_trainable = True\n",
    "sb_trainable = True\n",
    "\n",
    "kan_model = CustomKANModel(in_dim=in_dim, inter_dim=inter_dim, out_dim=out_dim, num_intervals=num_intervals, k=k, noise_scale=noise_scale, \n",
    "                            scale_base=scale_base, scale_sp=scale_sp, base_fun=base_fun, grid_eps=grid_eps, grid_range=grid_range, \n",
    "                            sp_trainable=sp_trainable, sb_trainable=sb_trainable, device=device).to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(kan_model.parameters(), lr=0.01)\n",
    "\n",
    "# Sparsification parameters\n",
    "lambda_l1 = 1e-4\n",
    "lambda_entropy = 1e-4\n",
    "\n",
    "# Helper function to compute sparsification penalties\n",
    "def compute_sparsification_penalty(model):\n",
    "    l1_penalty = 0.0\n",
    "    entropy_penalty = 0.0\n",
    "    for param in model.parameters():\n",
    "        l1_penalty += torch.sum(torch.abs(param))\n",
    "        param_normalized = torch.abs(param) / torch.sum(torch.abs(param))\n",
    "        entropy_penalty -= torch.sum(param_normalized * torch.log(param_normalized + 1e-10))\n",
    "    return lambda_l1 * l1_penalty + lambda_entropy * entropy_penalty\n",
    "\n",
    "# Training loop with sparsification\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    kan_model.train()\n",
    "    train_losses = []\n",
    "    train_correct = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    # Training phase\n",
    "    for X_batch, y_batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False):\n",
    "        # Forward pass\n",
    "        y_pred = kan_model(X_batch)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        \n",
    "        # Compute sparsification penalty\n",
    "        sparsification_penalty = compute_sparsification_penalty(kan_model)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = loss + sparsification_penalty\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(total_loss.item())\n",
    "        _, predicted = torch.max(y_pred.data, 1)\n",
    "        total_train += y_batch.size(0)\n",
    "        train_correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    train_accuracy = 100 * train_correct / total_train\n",
    "    \n",
    "    # Testing phase\n",
    "    kan_model.eval()\n",
    "    test_losses = []\n",
    "    test_correct = 0\n",
    "    total_test = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            y_pred = kan_model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            test_losses.append(loss.item())\n",
    "            _, predicted = torch.max(y_pred.data, 1)\n",
    "            total_test += y_batch.size(0)\n",
    "            test_correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    test_accuracy = 100 * test_correct / total_test\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {np.mean(train_losses):.4f}, Train Accuracy: {train_accuracy:.2f}%, Test Loss: {np.mean(test_losses):.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "# Pruning function\n",
    "def prune_kan_layer(kan_layer, threshold=1e-2):\n",
    "    with torch.no_grad():\n",
    "        for name, param in kan_layer.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                param *= (torch.abs(param) > threshold).float()\n",
    "\n",
    "# Prune the KAN layers after training\n",
    "prune_kan_layer(kan_model.kan_layer1)\n",
    "prune_kan_layer(kan_model.kan_layer2)\n",
    "\n",
    "# Retrain the pruned KAN model\n",
    "for epoch in range(num_epochs):\n",
    "    kan_model.train()\n",
    "    train_losses = []\n",
    "    train_correct = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    # Training phase\n",
    "    for X_batch, y_batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False):\n",
    "        # Forward pass\n",
    "        y_pred = kan_model(X_batch)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        \n",
    "        # Compute sparsification penalty\n",
    "        sparsification_penalty = compute_sparsification_penalty(kan_model)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = loss + sparsification_penalty\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(total_loss.item())\n",
    "        _, predicted = torch.max(y_pred.data, 1)\n",
    "        total_train += y_batch.size(0)\n",
    "        train_correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    train_accuracy = 100 * train_correct / total_train\n",
    "    \n",
    "    # Testing phase\n",
    "    kan_model.eval()\n",
    "    test_losses = []\n",
    "    test_correct = 0\n",
    "    total_test = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            y_pred = kan_model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            test_losses.append(loss.item())\n",
    "            _, predicted = torch.max(y_pred.data, 1)\n",
    "            total_test += y_batch.size(0)\n",
    "            test_correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    test_accuracy = 100 * test_correct / total_test\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {np.mean(train_losses):.4f}, Train Accuracy: {train_accuracy:.2f}%, Test Loss: {np.mean(test_losses):.4f}, Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4736783f-a7dd-4562-a912-cafeeb50ee4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "trainer = KANModelTrainer(mouse_number=715093703, output_dir=\"output\", timesteps_per_frame=1, batch_size=16, num_epochs=100)\n",
    "trainer.set_hyperparameters(inter_dim=118, num_layers=3, lr=0.01)\n",
    "trainer.prepare_data()\n",
    "trainer.create_kan_model()\n",
    "\n",
    "# Number of repetitions\n",
    "num_repetitions = 3\n",
    "\n",
    "# Repeat training and pruning\n",
    "for i in range(num_repetitions):\n",
    "    print(f\"Repetition {i+1}/{num_repetitions}\")\n",
    "    trainer.train()\n",
    "    trainer.prune_and_retrain()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
